{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "54bf6974-56fe-490b-9b06-61bd6634a710",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import csv\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.tokenize import word_tokenize,wordpunct_tokenize\n",
    "from nltk.collocations import BigramCollocationFinder, BigramAssocMeasures, TrigramCollocationFinder, TrigramAssocMeasures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d91501f-4ea8-4a51-80e4-5b2c3d87f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"/home/lucas-jerusalimiec/Documents/OCR Text/Notebooks\")\n",
    "from tokenizer_func  import (wordcleaner, write_words_to_file, dictionary_to_file, convert_tuple_bigrams,\n",
    "convert_tuple_trigrams)\n",
    "\n",
    "from extra_token_func import print_first_n_items, remove_keys_from_nested_dict\n",
    "\n",
    "from additional_token_func import convert_strings_to_counts\n",
    "\n",
    "from dict_write import write_dict_to_files_with_suffix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4d557ac9-168b-4b35-ac00-28687dd85cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./tokenized/\n",
      "Text files in the spellchecked directory: ['final/Discours des raisons_corrected.txt']\n"
     ]
    }
   ],
   "source": [
    "text_loc = Path(\"./final\")\n",
    "text_files = glob.glob(f\"{text_loc}/*.txt\")\n",
    "output_folder = './tokenized/'\n",
    "tokenized_folder = Path(output_folder)\n",
    "tokenized_folder.mkdir(exist_ok=True)\n",
    "print(output_folder)\n",
    "print(\"Text files in the spellchecked directory:\", text_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fa6226b-1236-4eb7-b4b3-7b8c79c0a6ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['w', 'iij', 'enl', 'ilz', 'ung', 'esté', 'seroit', 'mp', 'am', 'lx']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Open stopwords CSV file and list the contents\n",
    "with open('./stop_words.csv', 'r') as f:\n",
    "    stopwords = f.read().strip().split(\",\")\n",
    "stopwords[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b085009a-d824-4bf2-aedc-fd567abf2dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw texts: ['Discours des raisons_corrected']\n"
     ]
    }
   ],
   "source": [
    "tokenized_texts = {}\n",
    "for txt in text_files:\n",
    "    with open(txt, 'r') as f:\n",
    "        content = f.read()\n",
    "        file_name = txt.split('\\\\')[-1]\n",
    "        #key = file_name.split('.')[0]\n",
    "        key = os.path.splitext(os.path.basename(file_name))[0]\n",
    "        tokenized_texts[key] = content\n",
    "print(\"Raw texts:\", list(tokenized_texts.keys()))       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6f8cc71e-5849-4195-8b5c-585cc06d2916",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved content for 'Discours des raisons_corrected' to ./tokenized/Discours des raisons_corrected.txt\n"
     ]
    }
   ],
   "source": [
    "unigrams = {}\n",
    "\n",
    "for key, value in tokenized_texts.items():\n",
    "    unigram_list = wordpunct_tokenize(value)\n",
    "    cleanwords = [wordcleaner(w) for w in unigram_list]\n",
    "    unigrams[key] = cleanwords\n",
    "\n",
    "for key, value in unigrams.items():\n",
    "    filename = f\"./tokenized/{key}.txt\"\n",
    "    write_words_to_file(value, filename, words_per_line=20)\n",
    "    print(f\"Saved content for '{key}' to {filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d044c3f8-9aab-4362-8b85-a3709e228a06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram texts:\n",
      "Discours des raisons_corrected\n"
     ]
    }
   ],
   "source": [
    "print(\"Unigram texts:\")\n",
    "for key in unigrams:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51492bb5-b662-4e41-bf0d-35e2bd3e3e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigram Counts:\n",
      "Discours des raisons_corrected\n",
      "First 25 items in Discours des raisons_corrected:\n",
      ": 1828\n",
      "et: 441\n",
      "de: 344\n",
      "la: 233\n",
      "à: 167\n",
      "le: 163\n",
      "les: 159\n",
      "que: 156\n",
      "l: 123\n",
      "est: 120\n",
      "qui: 98\n",
      "en: 97\n",
      "ne: 87\n",
      "d: 81\n",
      "qu: 80\n",
      "ce: 76\n",
      "plus: 74\n",
      "ilz: 67\n",
      "il: 64\n",
      "du: 64\n",
      "des: 62\n",
      "par: 60\n",
      "leur: 60\n",
      "si: 53\n",
      "pour: 52\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Count up the tokens using a Counter() object\n",
    "unigram_counts = {}\n",
    "for key, value in unigrams.items():\n",
    "    unigram_counts_dict = Counter(value)\n",
    "    unigram_counts[key] = unigram_counts_dict\n",
    "\n",
    "print(\"Unigram Counts:\")\n",
    "for key in unigram_counts:\n",
    "    print(key)\n",
    "\n",
    "print_first_n_items(unigram_counts, 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1300638a-16da-4c7f-8e9c-1de304b45009",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 30 items in Discours des raisons_corrected:\n",
      "plus: 74\n",
      "si: 53\n",
      "leurs: 52\n",
      "roy: 47\n",
      "mémoires: 36\n",
      "sans: 25\n",
      "comme: 24\n",
      "hommes: 22\n",
      "tous: 22\n",
      "tant: 22\n",
      "ceulx: 21\n",
      "ceste: 21\n",
      "aultres: 20\n",
      "bien: 20\n",
      "tout: 19\n",
      "paix: 18\n",
      "mal: 18\n",
      "dieu: 18\n",
      "guerre: 17\n",
      "contre: 17\n",
      "estat: 17\n",
      "faire: 16\n",
      "aultre: 16\n",
      "toutes: 15\n",
      "eulx: 15\n",
      "prince: 15\n",
      "ainsy: 15\n",
      "subjects: 15\n",
      "estre: 14\n",
      "liberté: 14\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Remove specified keys from the dictionary\n",
    "stripped_unigrams = remove_keys_from_nested_dict(unigram_counts, stopwords)\n",
    "\n",
    "print_first_n_items(stripped_unigrams, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2d63ded-ea90-4894-af13-c8373b235afe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Discours des raisons_corrected_unigram_counts.csv in ./tokenized/\n"
     ]
    }
   ],
   "source": [
    "dictionary_to_file(stripped_unigrams, output_folder, 'unigram_counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4d6661b6-baf5-47d8-a58b-49bfedc0a32c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigrams:\n",
      "Discours des raisons_corrected\n"
     ]
    }
   ],
   "source": [
    "bigrams = {}\n",
    "\n",
    "for key, value in unigrams.items():\n",
    "    unigram_list = [word for word in value if word.lower() not in stopwords]\n",
    "    #unigram_list = [word for word in value]\n",
    "    bigrams_list = list(nltk.bigrams(unigram_list))\n",
    "    bigrams[key] = bigrams_list\n",
    "\n",
    "print(\"Bigrams:\")\n",
    "for key in bigrams:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24b278bb-d5b9-436c-8a31-3d626532ca5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram Counts:\n",
      "Discours des raisons_corrected\n",
      "First 30 items in Discours des raisons_corrected:\n",
      "toutes choses: 5\n",
      "peult estre: 4\n",
      "princes peuples: 4\n",
      "bon droict: 4\n",
      "tout ainsy: 4\n",
      "ainsy disoit: 4\n",
      "donner loy: 4\n",
      "peuple romain: 4\n",
      "roy plus: 3\n",
      "plus grand: 3\n",
      "tous ceulx: 3\n",
      "cy devant: 3\n",
      "aultre costé: 3\n",
      "aujourd huy: 3\n",
      "plus fort: 3\n",
      "si roy: 3\n",
      "aultre chose: 3\n",
      "non seulement: 3\n",
      "guerre paix: 2\n",
      "point suivi: 2\n",
      "toutes aultres: 2\n",
      "aultres choses: 2\n",
      "tous princes: 2\n",
      "plus juste: 2\n",
      "leurs entreprinses: 2\n",
      "sans discipline: 2\n",
      "avecque eulx: 2\n",
      "leurs vies: 2\n",
      "maisons femmes: 2\n",
      "femmes enfans: 2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "bigram_counts = {}\n",
    "\n",
    "for key, value in bigrams.items():\n",
    "    string_bigrams = convert_tuple_bigrams(value)\n",
    "    bigramCount = convert_strings_to_counts(string_bigrams)\n",
    "    bigram_counts[key] = bigramCount\n",
    "\n",
    "print(\"Bigram Counts:\")\n",
    "for key in bigram_counts:\n",
    "    print(key)\n",
    "\n",
    "print_first_n_items(bigram_counts, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c4eb003-3c3c-4d9e-b5e9-1979bd1a3fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Discours des raisons_corrected_bigram_counts.csv in ./tokenized/\n"
     ]
    }
   ],
   "source": [
    "dictionary_to_file(bigram_counts, output_folder, 'bigram_counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "dfe978cd-c9e2-46c6-9c37-fbb524fbbd48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigrams:\n",
      "Discours des raisons_corrected\n"
     ]
    }
   ],
   "source": [
    "trigrams = {}\n",
    "\n",
    "for key, value in unigrams.items():\n",
    "    unigram_list = [word for word in value if word.lower() not in stopwords]\n",
    "    #unigram_list = [word for word in value]\n",
    "    trigrams_list = list(nltk.trigrams(unigram_list))\n",
    "    trigrams[key] = trigrams_list\n",
    "\n",
    "print(\"Trigrams:\")\n",
    "for key in bigrams:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0b2a5667-7557-49d9-9904-184526684891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram Counts:\n",
      "Discours des raisons_corrected\n",
      "First 30 items in Discours des raisons_corrected:\n",
      "tous princes peuples: 2\n",
      "maisons femmes enfans: 2\n",
      "roy peult estre: 2\n",
      "sorte si guerre: 2\n",
      "donner loy subjects: 2\n",
      "discours raisons persuasions: 1\n",
      "raisons persuasions paix: 1\n",
      "persuasions paix an: 1\n",
      "paix an but: 1\n",
      "an but guerre: 1\n",
      "but guerre paix: 1\n",
      "guerre paix laquelle: 1\n",
      "paix laquelle sacquiert: 1\n",
      "laquelle sacquiert composition: 1\n",
      "sacquiert composition pleine: 1\n",
      "composition pleine entiére: 1\n",
      "pleine entiére victoire: 1\n",
      "entiére victoire voix: 1\n",
      "victoire voix composition: 1\n",
      "voix composition semble: 1\n",
      "composition semble mal: 1\n",
      "semble mal séante: 1\n",
      "mal séante deffiance: 1\n",
      "séante deffiance réciproque: 1\n",
      "deffiance réciproque mutitelles: 1\n",
      "réciproque mutitelles haines: 1\n",
      "mutitelles haines injures: 1\n",
      "haines injures sub: 1\n",
      "injures sub sistance: 1\n",
      "sub sistance deux: 1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trigram_counts = {}\n",
    "\n",
    "for key, value in trigrams.items():\n",
    "    string_trigrams = convert_tuple_trigrams(value)\n",
    "    trigramCount = convert_strings_to_counts(string_trigrams)\n",
    "    trigram_counts[key] = trigramCount\n",
    "\n",
    "print(\"Trigram Counts:\")\n",
    "for key in trigram_counts:\n",
    "    print(key)\n",
    "    \n",
    "print_first_n_items(trigram_counts, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d36ad6c4-4f5b-4d15-b463-25318d4095b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved Discours des raisons_corrected_trigram_counts.csv in ./tokenized/\n"
     ]
    }
   ],
   "source": [
    "dictionary_to_file(trigram_counts, output_folder, 'trigram_counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "58fd2be9-d9ed-4c35-b058-5a1f944e992a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collocations:\n",
      "Discours des raisons_corrected\n",
      "Collocation Counts:\n",
      "Discours des raisons_corrected\n",
      "toutes choses 5\n",
      "peult estre 4\n",
      "princes peuples 4\n",
      "bon droict 4\n",
      "tout ainsy 4\n",
      "ainsy disoit 4\n",
      "donner loy 4\n",
      "peuple romain 4\n",
      "roy plus 3\n",
      "plus grand 3\n",
      "tous ceulx 3\n",
      "cy devant 3\n",
      "aultre costé 3\n",
      "aujourd huy 3\n",
      "plus fort 3\n",
      "si roy 3\n",
      "aultre chose 3\n",
      "non seulement 3\n",
      "\n",
      "Saved Discours des raisons_corrected_collocation_counts.csv in ./tokenized/\n"
     ]
    }
   ],
   "source": [
    "colloc_dict = {}\n",
    "colloc_counts = {}\n",
    "\n",
    "for key, value in unigrams.items():\n",
    "    unigram_list = [word for word in value if word.lower() not in stopwords]\n",
    "    bigram_finder = BigramCollocationFinder.from_words(unigram_list)\n",
    "    bigram_finder.apply_freq_filter(3)  # Make sure all collocations have occurred at least 5 times\n",
    "    collocations = bigram_finder.nbest(BigramAssocMeasures.pmi, 500)\n",
    "    colloc_dict[key] = collocations\n",
    "    \n",
    "    # Initialize Counter for colloc_counts\n",
    "    bigram_count_dict = Counter()\n",
    "\n",
    "    # Count the occurrences of each bigram in the text\n",
    "    bigram_finder = BigramCollocationFinder.from_words(unigram_list)\n",
    "    bigram_freqs = bigram_finder.ngram_fd.items()\n",
    "    \n",
    "    # Filter bigram counts based on collocations\n",
    "    for bigram, count in bigram_freqs:\n",
    "        if bigram in collocations:\n",
    "            bigram_count_dict[bigram] = count\n",
    "\n",
    "    colloc_counts[key] = bigram_count_dict\n",
    "\n",
    "print(\"Collocations:\")\n",
    "for key, value in colloc_dict.items():\n",
    "    print(key)\n",
    "    # for w1, w2 in value:\n",
    "    #     print(' ', w1, w2)\n",
    "\n",
    "print(\"Collocation Counts:\")\n",
    "for key in colloc_counts:\n",
    "    print(key)\n",
    "    # Print first n items, assuming print_first_n_items function is defined elsewhere\n",
    "    for item, count in colloc_counts[key].most_common(30):\n",
    "        bigram = \" \".join(item)\n",
    "        print(f\"{bigram} {count}\")\n",
    "    print()\n",
    "\n",
    "dictionary_to_file(colloc_counts, output_folder, 'collocation_counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d4f5f4c8-a3e2-4e10-9a2f-b3369eced597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram Collocations:\n",
      "Discours des raisons_corrected\n",
      "Trigram Collocation Counts:\n",
      "Discours des raisons_corrected\n",
      "\n",
      "Saved Discours des raisons_corrected_trigram_collocation_counts.csv in ./tokenized/\n"
     ]
    }
   ],
   "source": [
    "trigram_colloc_dict = {}\n",
    "trigram_colloc_counts = {}\n",
    "\n",
    "for key, value in unigrams.items():\n",
    "    unigram_list = [word for word in value if word.lower() not in stopwords]\n",
    "    trigram_finder = TrigramCollocationFinder.from_words(unigram_list)\n",
    "    trigram_finder.apply_freq_filter(3)  # Ensure all collocations have occurred at least 5 times\n",
    "    collocations = trigram_finder.nbest(TrigramAssocMeasures.pmi, 500)\n",
    "    trigram_colloc_dict[key] = collocations\n",
    "    \n",
    "    # Initialize Counter for trigram_colloc_counts\n",
    "    trigram_count_dict = Counter()\n",
    "\n",
    "    # Count the occurrences of each trigram in the text\n",
    "    trigram_freqs = trigram_finder.ngram_fd.items()\n",
    "    \n",
    "    # Filter trigram counts based on collocations\n",
    "    for trigram, count in trigram_freqs:\n",
    "        if trigram in collocations:\n",
    "            trigram_count_dict[trigram] = count\n",
    "\n",
    "    trigram_colloc_counts[key] = trigram_count_dict\n",
    "\n",
    "print(\"Trigram Collocations:\")\n",
    "for key, value in trigram_colloc_dict.items():\n",
    "    print(key)\n",
    "    #for w1, w2, w3 in value:\n",
    "    #    print(' ', w1, w2, w3)\n",
    "\n",
    "print(\"Trigram Collocation Counts:\")\n",
    "for key in trigram_colloc_counts:\n",
    "    print(key)\n",
    "    # Print first n items, assuming print_first_n_items function is defined elsewhere\n",
    "    for item, count in trigram_colloc_counts[key].most_common(30):\n",
    "        trigram = \" \".join(item)\n",
    "        print(f\"{trigram} {count}\")\n",
    "    print()\n",
    "\n",
    "dictionary_to_file(trigram_colloc_counts, output_folder, 'trigram_collocation_counts')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6b890221-5827-4d01-a9ab-9726fde00e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Underscore Dictionary:\n",
      "Discours des raisons_corrected\n",
      "Discours des raisons_corrected_underscore_bigrams.txt in ./tokenized/\n"
     ]
    }
   ],
   "source": [
    "underscore_dict = {}\n",
    "for key, value in unigrams.items():\n",
    "\n",
    "    tokenized_words = unigrams.get(key)\n",
    "    collocations = colloc_dict.get(key)\n",
    "    \n",
    "    colloc_words = []\n",
    "    \n",
    "    # Iterate through the words making new versions combining collocations\n",
    "    i = 0\n",
    "    while i < len(tokenized_words) - 1:\n",
    "        # If we find a collocation, add and advance by two words\n",
    "        if (tokenized_words[i], tokenized_words[i + 1]) in collocations:\n",
    "            colloc_words.append('_'.join((tokenized_words[i], tokenized_words[i + 1])))\n",
    "            i += 2\n",
    "        # Otherwise, advance by one word\n",
    "        else:\n",
    "            colloc_words.append(tokenized_words[i])\n",
    "            i += 1\n",
    "\n",
    "    # Add the last word (if any)\n",
    "    if i == len(tokenized_words) - 1:\n",
    "        colloc_words.append(tokenized_words[i])\n",
    "    underscore_dict[key] = colloc_words\n",
    "\n",
    "print(\"Underscore Dictionary:\")\n",
    "for key in underscore_dict:\n",
    "    print(key)\n",
    "\n",
    "write_dict_to_files_with_suffix(underscore_dict, output_folder, 'underscore_bigrams')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "8f0b3b4c-c6df-4bdf-84c2-fb152e0b68e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trigram underscore Dictionary:\n",
      "Discours des raisons_corrected\n",
      "Discours des raisons_corrected_underscore_trigrams.txt in ./tokenized/\n"
     ]
    }
   ],
   "source": [
    "trigram_underscore_dict = {}\n",
    "\n",
    "for key, tokenized_words in unigrams.items():\n",
    "    collocations = trigram_colloc_dict.get(key, [])\n",
    "    colloc_words = []\n",
    "    i = 0\n",
    "    while i < len(tokenized_words) - 2:\n",
    "        # If we find a trigram collocation, add and advance by three words\n",
    "        trigram = (tokenized_words[i], tokenized_words[i + 1], tokenized_words[i + 2])\n",
    "        if trigram in collocations:\n",
    "            colloc_words.append('_'.join(trigram))\n",
    "            i += 3\n",
    "        else:\n",
    "            colloc_words.append(tokenized_words[i])\n",
    "            i += 1\n",
    "    # Add the last words (if any)\n",
    "    while i < len(tokenized_words):\n",
    "        colloc_words.append(tokenized_words[i])\n",
    "        i += 1\n",
    "    trigram_underscore_dict[key] = colloc_words\n",
    "\n",
    "print(\"Trigram underscore Dictionary:\")\n",
    "for key in trigram_underscore_dict:\n",
    "    print(key)\n",
    "\n",
    "write_dict_to_files_with_suffix(trigram_underscore_dict, output_folder, 'underscore_trigrams')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

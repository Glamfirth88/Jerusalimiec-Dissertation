{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae2df9b-3aa2-4b68-b1a1-c426be6a30bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select your stopwords .csv:\n",
      "1. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Sectionized/lemmatized/stop_words.csv\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select RATE DICTIONARY directory:\n",
      "1. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Sectionized/lemmatized/concordances\n",
      "2. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Sectionized/lemmatized/.ipynb_checkpoints\n",
      "3. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Sectionized/lemmatized\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available files:\n",
      "1. Discours des raisons_corrected_stemmed.txt\n",
      "2. Démonomanie I.1_corrected_stemmed.txt\n",
      "3. Démonomanie I.2_corrected_stemmed.txt\n",
      "4. Démonomanie I.3_corrected_stemmed.txt\n",
      "5. Démonomanie I.4_corrected_stemmed.txt\n",
      "6. Démonomanie I.5_corrected_stemmed.txt\n",
      "7. Démonomanie I.6_corrected_stemmed.txt\n",
      "8. Démonomanie I.7_corrected_stemmed.txt\n",
      "9. Démonomanie II.1_corrected_stemmed.txt\n",
      "10. Démonomanie II.2_corrected_stemmed.txt\n",
      "11. Démonomanie II.3_corrected_stemmed.txt\n",
      "12. Démonomanie II.4_corrected_stemmed.txt\n",
      "13. Démonomanie II.5_corrected_stemmed.txt\n",
      "14. Démonomanie II.6_corrected_stemmed.txt\n",
      "15. Démonomanie II.7_corrected_stemmed.txt\n",
      "16. Démonomanie II.8_corrected_stemmed.txt\n",
      "17. Démonomanie III.1_corrected_stemmed.txt\n",
      "18. Démonomanie III.2_corrected_stemmed.txt\n",
      "19. Démonomanie III.3_corrected_stemmed.txt\n",
      "20. Démonomanie III.4_corrected_stemmed.txt\n",
      "21. Démonomanie III.5_corrected_stemmed.txt\n",
      "22. Démonomanie III.6_corrected_stemmed.txt\n",
      "23. Démonomanie IV.1_corrected_stemmed.txt\n",
      "24. Démonomanie IV.2_corrected_stemmed.txt\n",
      "25. Démonomanie IV.3_corrected_stemmed.txt\n",
      "26. Démonomanie IV.4_corrected_stemmed.txt\n",
      "27. Démonomanie IV.5_corrected_stemmed.txt\n",
      "28. Démonomanie preface Repair_corrected_stemmed.txt\n",
      "29. Harangue - Fontainebleau_corrected_stemmed.txt\n",
      "30. Harangue - Orléans 2_corrected_stemmed.txt\n",
      "31. Harangue - Orléans_corrected_stemmed.txt\n",
      "32. Harangue - Poissy_corrected_stemmed.txt\n",
      "33. Harangue - Rouen_corrected_stemmed.txt\n",
      "34. Harangue - Saint Germain_corrected_stemmed.txt\n",
      "35. Harangue - lit de justice_corrected_stemmed.txt\n",
      "36. Harangue - ouverture de parlement_corrected_stemmed.txt\n",
      "37. Harangue - parlement 2_corrected_stemmed.txt\n",
      "38. Harangue - parlement 3_corrected_stemmed.txt\n",
      "39. Harangue - parlement_corrected_stemmed.txt\n",
      "40. Harangue - religion_corrected_stemmed.txt\n",
      "41. Harangue - septembre_corrected_stemmed.txt\n",
      "42. La réponse_corrected_stemmed.txt\n",
      "43. Le paradoxe_corrected_stemmed.txt\n",
      "44. Lettre_corrected_stemmed.txt\n",
      "45. Lit de justice_corrected_stemmed.txt\n",
      "46. Memoire - Namur_corrected_stemmed.txt\n",
      "47. Memoire - le but_corrected_stemmed.txt\n",
      "48. Memoire au roi_corrected_stemmed.txt\n",
      "49. Memoires d'État Refuge_corrected_stemmed.txt\n",
      "50. Memoires d'état_corrected_stemmed.txt\n",
      "51. Recueil_corrected_stemmed.txt\n",
      "52. Remonstrances - Royaume_corrected_stemmed.txt\n",
      "53. Remonstrances - parlement_corrected_stemmed.txt\n",
      "54. République I.1_corrected_stemmed.txt\n",
      "55. République I.2_corrected_stemmed.txt\n",
      "56. République I.3_corrected_stemmed.txt\n",
      "57. République I.4_corrected_stemmed.txt\n",
      "58. République I.5_corrected_stemmed.txt\n",
      "59. République I.6_corrected_stemmed.txt\n",
      "60. République I.7_corrected_stemmed.txt\n",
      "61. République I.8_corrected_stemmed.txt\n",
      "62. République I.910_corrected_stemmed.txt\n",
      "63. République I.911_corrected_stemmed.txt\n",
      "64. République I.9_corrected_stemmed.txt\n",
      "65. République II.1_corrected_stemmed.txt\n",
      "66. République II.2_corrected_stemmed.txt\n",
      "67. République II.3_corrected_stemmed.txt\n",
      "68. République II.4_corrected_stemmed.txt\n",
      "69. République II.5_corrected_stemmed.txt\n",
      "70. République II.6_corrected_stemmed.txt\n",
      "71. République II.7_corrected_stemmed.txt\n",
      "72. République III.1_corrected_stemmed.txt\n",
      "73. République III.2_corrected_stemmed.txt\n",
      "74. République III.3_corrected_stemmed.txt\n",
      "75. République III.4_corrected_stemmed.txt\n",
      "76. République III.5_corrected_stemmed.txt\n",
      "77. République III.6_corrected_stemmed.txt\n",
      "78. République III.7_corrected_stemmed.txt\n",
      "79. République IV.1_corrected_stemmed.txt\n",
      "80. République IV.2_corrected_stemmed.txt\n",
      "81. République IV.3_corrected_stemmed.txt\n",
      "82. République IV.4_corrected_stemmed.txt\n",
      "83. République IV.5_corrected_stemmed.txt\n",
      "84. République IV.6_corrected_stemmed.txt\n",
      "85. République IV.7_corrected_stemmed.txt\n",
      "86. République V.1_corrected_stemmed.txt\n",
      "87. République V.2_corrected_stemmed.txt\n",
      "88. République V.3_corrected_stemmed.txt\n",
      "89. République V.4_corrected_stemmed.txt\n",
      "90. République V.5_corrected_stemmed.txt\n",
      "91. République VI.1_corrected_stemmed.txt\n",
      "92. République VI.2_corrected_stemmed.txt\n",
      "93. République VI.3_corrected_stemmed.txt\n",
      "94. République VI.4_corrected_stemmed.txt\n",
      "95. République VI.5_corrected_stemmed.txt\n",
      "96. République VI.6_corrected_stemmed.txt\n",
      "97. République preface_corrected_stemmed.txt\n",
      "98. Théatre III_corrected_stemmed.txt\n",
      "99. Théatre II_corrected_stemmed.txt\n",
      "100. Théatre IV_corrected_stemmed.txt\n",
      "101. Théatre I_corrected_stemmed.txt\n",
      "102. Théatre V_corrected_stemmed.txt\n",
      "103. Théatre summary_corrected_stemmed.txt\n",
      "104. Traite Justice VII_corrected_stemmed.txt\n",
      "105. Traite Justice VI_corrected_stemmed.txt\n",
      "106. Traite Justice V_corrected_stemmed.txt\n",
      "107. Traité Justice III_corrected_stemmed.txt\n",
      "108. Traité Justice II_corrected_stemmed.txt\n",
      "109. Traité Justice IV_corrected_stemmed.txt\n",
      "110. Traité Justice I_corrected_stemmed.txt\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Pick files (numbers, ranges e.g. 1-3, or prefix text):  all\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select TARGET FILES directory:\n",
      "1. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Sectionized/lemmatized/concordances\n",
      "2. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Sectionized/lemmatized/.ipynb_checkpoints\n",
      "3. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Sectionized/lemmatized\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 290\u001b[0m\n\u001b[1;32m    286\u001b[0m     display_success_message()\n\u001b[1;32m    289\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 290\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 254\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    251\u001b[0m rate_dict \u001b[38;5;241m=\u001b[39m calculate_rate_dictionary(rate_files, rate_dir)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;66;03m# Change 6: select files to topic‐model\u001b[39;00m\n\u001b[0;32m--> 254\u001b[0m target_dir \u001b[38;5;241m=\u001b[39m \u001b[43mchoose_directory\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSelect TARGET FILES directory:\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    255\u001b[0m target_files \u001b[38;5;241m=\u001b[39m choose_files(list_txt_files(target_dir))\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# Change 3: alpha threshold\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[1], line 60\u001b[0m, in \u001b[0;36mchoose_directory\u001b[0;34m(prompt)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m     59\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 60\u001b[0m         sel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mEnter number: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m sel \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(options):\n\u001b[1;32m     62\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m options[sel \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/ipykernel/kernelbase.py:1282\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1280\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1281\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1282\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/ipykernel/kernelbase.py:1325\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1322\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1323\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1324\u001b[0m     msg \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1325\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1326\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Integrated Jupyter‐style script for MALLET‐based topic modeling\n",
    "with per‐file Fisher’s Exact filtering and stopword exclusion.\n",
    "Supports UTF-8 encoding for French accents.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from scipy.stats import fisher_exact\n",
    "import little_mallet_wrapper\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "# Path to your MALLET binary\n",
    "# path_to_mallet = \"mallet-2.0.8/bin/mallet\"\n",
    "path_to_mallet = os.path.expanduser(\"~/mallet-2.0.8/bin/mallet\")\n",
    "\n",
    "\n",
    "# ─── Utility Functions ─────────────────────────────────────────────────────────\n",
    "\n",
    "def tokenize_file(filepath):\n",
    "    \"\"\"Read a text file (UTF-8) and split into tokens (preserves French accents).\"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read().split()\n",
    "\n",
    "\n",
    "def list_txt_files(directory):\n",
    "    \"\"\"Return sorted list of .txt filenames in a directory.\"\"\"\n",
    "    return sorted([fn for fn in os.listdir(directory) if fn.endswith(\".txt\")])\n",
    "\n",
    "\n",
    "def list_csv_files(directory):\n",
    "    \"\"\"Recursively find all .csv files under directory.\"\"\"\n",
    "    csvs = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for fn in files:\n",
    "            if fn.endswith(\".csv\"):\n",
    "                csvs.append(os.path.join(root, fn))\n",
    "    return csvs\n",
    "\n",
    "\n",
    "def choose_directory(prompt):\n",
    "    \"\"\"\n",
    "    Let user pick one of the subfolders (or current) by number.\n",
    "    Shows relative paths starting from the name of the current directory.\n",
    "    Returns a full path.\n",
    "    \"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    base = Path(cwd).name\n",
    "    # Collect current folder and all subdirectories\n",
    "    dirs = [cwd]\n",
    "    for root, subdirs, _ in os.walk(cwd):\n",
    "        for d in subdirs:\n",
    "            dirs.append(os.path.join(root, d))\n",
    "\n",
    "    # Build display names\n",
    "    options = []\n",
    "    for d in dirs:\n",
    "        rel = os.path.relpath(d, cwd)\n",
    "        display = base if rel == \".\" else f\"{base}/{rel}\"\n",
    "        options.append((display, d))\n",
    "\n",
    "    print(prompt)\n",
    "    for i, (display, _) in enumerate(options, start=1):\n",
    "        print(f\"{i}. {display}\")\n",
    "    while True:\n",
    "        try:\n",
    "            sel = int(input(\"Enter number: \").strip())\n",
    "            if 1 <= sel <= len(options):\n",
    "                return options[sel - 1][1]\n",
    "        except ValueError:\n",
    "            pass\n",
    "        print(\"Invalid choice, try again.\")\n",
    "\n",
    "\n",
    "def choose_files(files):\n",
    "    \"\"\"\n",
    "    Let user select from a numbered list of filenames.\n",
    "    Supports 'all', single number, ranges (1-3), or prefix patterns.\n",
    "    Displays selected files after selection.\n",
    "    \"\"\"\n",
    "    print(\"Available files:\")\n",
    "    for i, fn in enumerate(files, start=1):\n",
    "        print(f\"{i}. {fn}\")\n",
    "    choice = input(\n",
    "        \"Pick files ('all', numbers, ranges e.g. 1-3, or prefix text): \"\n",
    "    ).strip()\n",
    "    if choice.lower() == \"all\":\n",
    "        sel = files[:]\n",
    "    else:\n",
    "        sel = []\n",
    "        for part in choice.split(\",\"):\n",
    "            part = part.strip()\n",
    "            if \"-\" in part:\n",
    "                a, b = map(int, part.split(\"-\"))\n",
    "                sel.extend(files[a - 1 : b])\n",
    "            elif part.isdigit():\n",
    "                sel.append(files[int(part) - 1])\n",
    "            else:\n",
    "                sel.extend([fn for fn in files if fn.startswith(part)])\n",
    "        sel = sorted(set(sel))\n",
    "    print(\"Selected files:\")\n",
    "    for fn in sel:\n",
    "        print(f\"- {fn}\")\n",
    "    return sel\n",
    "\n",
    "\n",
    "def choose_csv_file(csv_files):\n",
    "    \"\"\"Prompt user to pick one CSV file from a list.\"\"\"\n",
    "    print(\"Select your stopwords .csv:\")\n",
    "    for i, path in enumerate(csv_files, start=1):\n",
    "        print(f\"{i}. {path}\")\n",
    "    while True:\n",
    "        try:\n",
    "            sel = int(input(\"Enter number: \").strip())\n",
    "            if 1 <= sel <= len(csv_files):\n",
    "                return csv_files[sel - 1]\n",
    "        except ValueError:\n",
    "            pass\n",
    "        print(\"Invalid choice.\")\n",
    "\n",
    "\n",
    "def read_stopwords(filepath):\n",
    "    \"\"\"Read CSV stopword file (UTF-8), split on commas, strip whitespace.\"\"\"\n",
    "    sw = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            for cell in row:\n",
    "                sw.extend(cell.split(\",\"))\n",
    "    return [w.strip() for w in sw if w.strip()]\n",
    "\n",
    "\n",
    "# ─── Fisher’s Exact & Rate Dictionary ──────────────────────────────────────────\n",
    "\n",
    "def get_fishers(word, freq_dict, rate_dict, alternative=\"greater\"):\n",
    "    \"\"\"\n",
    "    Compute p-value of Fisher’s Exact Test:\n",
    "      [[obs, total-obs],\n",
    "       [exp, total-exp]]\n",
    "    \"\"\"\n",
    "    obs = freq_dict.get(word, 0)\n",
    "    total = sum(freq_dict.values())\n",
    "    remainder = total - obs\n",
    "    exp = round(rate_dict.get(word, 0) * total)\n",
    "    comp_exp = total - exp\n",
    "    _, pval = fisher_exact(\n",
    "        [[obs, remainder], [exp, comp_exp]], alternative=alternative\n",
    "    )\n",
    "    return pval\n",
    "\n",
    "\n",
    "def calculate_rate_dictionary(rate_files, rate_dir):\n",
    "    \"\"\"\n",
    "    Build a global rate dictionary from a set of reference files.\n",
    "    Returns { token: relative_frequency }.\n",
    "    \"\"\"\n",
    "    ctr = Counter()\n",
    "    total = 0\n",
    "    for fn in rate_files:\n",
    "        tokens = tokenize_file(os.path.join(rate_dir, fn))\n",
    "        ctr.update(tokens)\n",
    "        total += len(tokens)\n",
    "    return {tok: cnt / total for tok, cnt in ctr.items()}\n",
    "\n",
    "\n",
    "# ─── Preprocessing & Training Data Prep ───────────────────────────────────────\n",
    "\n",
    "def prepare_training_data(target_files, target_dir, stopwords, rate_dict, alpha):\n",
    "    \"\"\"\n",
    "    For each file:\n",
    "      – tokenize (UTF-8)\n",
    "      – build frequency dict\n",
    "      – run Fisher’s Exact per token against rate_dict\n",
    "      – keep only tokens with p-value < alpha and not in stopwords\n",
    "    Returns:\n",
    "      docs: [ str ]        # documents ready for MALLET\n",
    "      dists: [ dict ]      # raw freq-dicts per document\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    dists = []\n",
    "    for fn in target_files:\n",
    "        path = os.path.join(target_dir, fn)\n",
    "        tokens = tokenize_file(path)\n",
    "        freq = Counter(tokens)\n",
    "        filtered = [\n",
    "            w\n",
    "            for w in tokens\n",
    "            if w not in stopwords and get_fishers(w, freq, rate_dict) < alpha\n",
    "        ]\n",
    "        docs.append(\" \".join(filtered))\n",
    "        dists.append(freq)\n",
    "    return docs, dists\n",
    "\n",
    "\n",
    "# ─── Topic Model Training & Export ────────────────────────────────────────────\n",
    "\n",
    "def train_topic_model(training_data, num_topics, output_dir):\n",
    "    \"\"\"\n",
    "    Calls little_mallet_wrapper to train and load topics.\n",
    "    Creates output_dir if needed.\n",
    "    Returns: topics (list of lists)\n",
    "    \"\"\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    little_mallet_wrapper.quick_train_topic_model(\n",
    "        path_to_mallet, output_dir, num_topics, training_data\n",
    "    )\n",
    "    return little_mallet_wrapper.load_topic_keys(\n",
    "        f\"{output_dir}/mallet.topic_keys.{num_topics}\"\n",
    "    )\n",
    "\n",
    "\n",
    "def save_results_to_excel(excel_path, topics, distributions, target_files):\n",
    "    \"\"\"\n",
    "    1) Sheet 'Topics' with one row per topic (Topic#, tokens...)\n",
    "    2) One sheet per target_file (sheet name = stem) showing distribution\n",
    "    \"\"\"\n",
    "    wb = Workbook()\n",
    "    # -- Topics sheet --\n",
    "    ws0 = wb.active\n",
    "    ws0.title = \"Topics\"\n",
    "    for idx, topic in enumerate(topics):\n",
    "        ws0.append([f\"Topic {idx}\"] + topic)\n",
    "\n",
    "    # -- Per-document distribution sheets --\n",
    "    for fn, dist in zip(target_files, distributions):\n",
    "        stem = Path(fn).stem\n",
    "        ws = wb.create_sheet(title=stem)\n",
    "        df = pd.DataFrame.from_dict(dist, orient=\"index\", columns=[\"count\"])\n",
    "        for row in dataframe_to_rows(df, index=True, header=True):\n",
    "            ws.append(row)\n",
    "\n",
    "    wb.save(excel_path)\n",
    "\n",
    "\n",
    "def export_heatmap(model, output_dir):\n",
    "    \"\"\"\n",
    "    Plot and save the MALLET heatmap as JPG for Word insertion.\n",
    "    \"\"\"\n",
    "    fig = little_mallet_wrapper.plot_categories_by_topics_heatmap(model)\n",
    "    fig.savefig(os.path.join(output_dir, \"heatmap.jpg\"), format=\"jpg\", dpi=300)\n",
    "\n",
    "\n",
    "def save_top_titles(excel_path, n_titles, topics):\n",
    "    \"\"\"\n",
    "    Write top n_titles per topic into an .xlsx,\n",
    "    one sheet per topic named 'Topic{#}'.\n",
    "    \"\"\"\n",
    "    wb = Workbook()\n",
    "    for idx, _ in enumerate(topics):\n",
    "        ws = wb.create_sheet(title=f\"Topic{idx}\")\n",
    "        top = little_mallet_wrapper.display_top_titles_per_topic(idx, n_titles)\n",
    "        for title in top:\n",
    "            ws.append([title])\n",
    "    wb.save(excel_path)\n",
    "\n",
    "\n",
    "def display_success_message():\n",
    "    print(\"✅ Notebook has finished successfully!\")\n",
    "\n",
    "\n",
    "# ─── Main Workflow ────────────────────────────────────────────────────────────\n",
    "\n",
    "def main():\n",
    "    # Change 0: select stopwords list\n",
    "    stop_csv = choose_csv_file(list_csv_files(os.getcwd()))\n",
    "    stopwords = read_stopwords(stop_csv)\n",
    "\n",
    "    # Change 2: select files for rate dictionary\n",
    "    rate_dir = choose_directory(\"Select RATE DICTIONARY directory:\")\n",
    "    rate_files = choose_files(list_txt_files(rate_dir))\n",
    "    rate_dict = calculate_rate_dictionary(rate_files, rate_dir)\n",
    "\n",
    "    # Change 6: select files to topic‐model\n",
    "    target_dir = choose_directory(\"Select TARGET FILES directory:\")\n",
    "    target_files = choose_files(list_txt_files(target_dir))\n",
    "\n",
    "    # Change 3: alpha threshold\n",
    "    alpha = float(input(\"Enter alpha threshold for Fisher’s Exact: \").strip())\n",
    "\n",
    "    # Changes 4 & 5: prepare filtered training data\n",
    "    training_docs, distributions = prepare_training_data(\n",
    "        target_files, target_dir, stopwords, rate_dict, alpha\n",
    "    )\n",
    "\n",
    "    # Change 7 & 12: number of topics and output subfolder\n",
    "    num_topics = int(input(\"Enter number of topics to generate: \").strip())\n",
    "    out_sub = input(\"Enter name for output subfolder: \").strip()\n",
    "    output_dir = os.path.join(os.getcwd(), out_sub)\n",
    "\n",
    "    # Train model (Change 11 & 12)\n",
    "    topics = train_topic_model(training_docs, num_topics, output_dir)\n",
    "\n",
    "    # Export to Excel (Changes 13–15)\n",
    "    excel_path = os.path.join(output_dir, \"topic_model_results.xlsx\")\n",
    "    save_results_to_excel(excel_path, topics, distributions, target_files)\n",
    "\n",
    "    # Export heatmap as JPG (Change 16)\n",
    "    export_heatmap(topics, output_dir)\n",
    "\n",
    "    # Changes 17–19: top‐titles export\n",
    "    n_titles = int(input(\"How many top titles per topic? \").strip())\n",
    "    top_titles_path = os.path.join(output_dir, \"top_titles.xlsx\")\n",
    "    save_top_titles(top_titles_path, n_titles, topics)\n",
    "\n",
    "    # Change 20: success message\n",
    "    display_success_message()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

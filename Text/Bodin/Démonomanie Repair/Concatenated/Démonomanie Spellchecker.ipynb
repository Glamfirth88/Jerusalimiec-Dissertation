{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcd07665-38bc-4fed-8621-4a04acb65976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import glob, a module that helps with file management.\n",
    "import glob\n",
    "import pandas as pd\n",
    "from spellchecker import SpellChecker\n",
    "from pathlib import Path\n",
    "from nltk import wordpunct_tokenize\n",
    "from collections import Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27b7fc5d-c697-4ef5-ac4d-c6345a5936cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text files to be spellchecked: ['Démonomanie Repair.txt']\n",
      "doc_name = 'Démonomanie Repair'\n"
     ]
    }
   ],
   "source": [
    "#doc_name = 'Démonomanie'\n",
    "# Define the output path and create the directory if it doesn't exist\n",
    "outputpath = \"./final\"\n",
    "outputfile_path = Path(outputpath)\n",
    "outputfile_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Get the current working directory\n",
    "texts_folder = Path.cwd()\n",
    "\n",
    "# Find all .txt files in the current directory\n",
    "texts_list = glob.glob(\"*.txt\")\n",
    "print(\"Text files to be spellchecked:\", texts_list)\n",
    "\n",
    "# Set doc_name to the name of the first .txt file found in texts_list (without the extension)\n",
    "if texts_list:\n",
    "    doc_name = Path(texts_list[0]).stem\n",
    "else:\n",
    "    doc_name = None\n",
    "\n",
    "print(f\"doc_name = '{doc_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8251a74-964b-47ac-9752-9d785cb06cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spellchecker dictionary.\n",
    "# Replace the language attribute with another 2 letter code\n",
    "# to select another language. Options are: English - ‘en’, Spanish - ‘es’,\n",
    "# French - ‘fr’, Portuguese - ‘pt’, German - ‘de’, Russian - ‘ru’.\n",
    "\n",
    "spell = SpellChecker(language='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d411299b-c6ea-4ac9-9bc5-97628f5fcff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lucas-jerusalimiec/Documents/OCR Text/Text/Bodin/Démonomanie Repair/Concatenated/Démonomanie Repair.txt checked for readability.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>token_count</th>\n",
       "      <th>unknown_count</th>\n",
       "      <th>readability</th>\n",
       "      <th>unknown_words</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>placeholder</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>placeholder</td>\n",
       "      <td>placeholder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/lucas-jerusalimiec/Documents/OCR Text/Te...</td>\n",
       "      <td>108605</td>\n",
       "      <td>22447</td>\n",
       "      <td>79.33</td>\n",
       "      <td>{jes, dóc, almai, patle, oftale, clauclee, àdi...</td>\n",
       "      <td>\\n\\n    E DEFINITION DV S; OR CLE. R- CHAPITRE...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_name  token_count  \\\n",
       "0                                        placeholder            0   \n",
       "0  /home/lucas-jerusalimiec/Documents/OCR Text/Te...       108605   \n",
       "\n",
       "   unknown_count  readability  \\\n",
       "0              0         0.00   \n",
       "0          22447        79.33   \n",
       "\n",
       "                                       unknown_words  \\\n",
       "0                                        placeholder   \n",
       "0  {jes, dóc, almai, patle, oftale, clauclee, àdi...   \n",
       "\n",
       "                                                text  \n",
       "0                                        placeholder  \n",
       "0  \\n\\n    E DEFINITION DV S; OR CLE. R- CHAPITRE...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Dictionary Test a Folder of .txt Files ###\n",
    "\n",
    "# We'll use Pandas to create a dataframe (a table) that can hold \n",
    "# information about an OCR'ed page and display it in a tabular format.\n",
    "# This dataframe will start out empty with only its column headers \n",
    "# defined. We'll add information to it one page at a time. So each\n",
    "# row will represent 1 page.\n",
    "\n",
    "placeholder_values = ['placeholder', 0, 0, 0, 'placeholder', 'placeholder']\n",
    "\n",
    "df = pd.DataFrame([placeholder_values], columns=[\"file_name\",\"token_count\",\"unknown_count\",\"readability\",\"unknown_words\",\"text\"])\n",
    "\n",
    "# Set the folder for the input images\n",
    "\n",
    "for txt_file in texts_folder.glob('*.txt'):\n",
    "    \n",
    "    # Open each text file and read text into `ocrText`\n",
    "    with open(txt_file, 'r') as inputFile:\n",
    "        ocrText = inputFile.read()\n",
    "        \n",
    "    # Join hyphenated words that are split between lines by \n",
    "    # looking for a hyphen followed by a newline character: \"-\\n\"\n",
    "    # \"\\n\" is an \"escape character\" and represents the \n",
    "    # \"newline,\" a character that is usually invisible \n",
    "    # to human readers but that computers use to mark the \n",
    "    # end/beginning of a line. Each time you press the \n",
    "    # Enter/Return key on your keyboard, an invisible \"\\n\" \n",
    "    # is created to mark the beginning of a new line.\n",
    "    ocrText = ocrText.replace(\"-\\n\",\"\")\n",
    "    \n",
    "    # First, we'll use NLTK to \"tokenize\" text. \n",
    "            # \"Tokenize\" here means to take a page of our OCR'ed text,\n",
    "            # which Python is currently reading as one big glob of data,\n",
    "            # and separate each word out so that it can be read as an\n",
    "            # individual piece of data within a larger data structure \n",
    "            # (a list). This process also removes punctuation.\n",
    "    tokens = wordpunct_tokenize(ocrText)\n",
    "    \n",
    "    # Lowercase all tokens\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    \n",
    "    # Now we can get all of the words that don't match the \n",
    "    # spellchecker dictionary or our list of place names--\n",
    "    # these are the potential spelling errors.\n",
    "    unknown = spell.unknown(tokens)\n",
    "    \n",
    "    # Let's use a little math to find out how many potential \n",
    "    # spelling errors were identified. As part of this process, \n",
    "    # we'll create a \"readability\" score that will give us a \n",
    "    # percentage of how readable each file is--how much of the \n",
    "    # OCR'ed is \"correct.\"\n",
    "        \n",
    "    # If the list of unknown tokens (words) is greater than 0 \n",
    "    # (i.e. if the list is not empty):\n",
    "    if len(unknown) != 0:\n",
    "            \n",
    "               # Following order of operations, here's what's happening \n",
    "               # in the readability variable below:\n",
    "               # 1. Divide the number of unknown tokens (len(unknown)) \n",
    "                    # by the total number of tokens on the page\n",
    "                    # (len(tokens)). Use \"float\" to specify that Python\n",
    "                    # returns a decimal number:\n",
    "                        # (float(len(unknown))/float(len(tokens))\n",
    "               # 2. Multiply the number from step 1 by 100.\n",
    "                    # (float(len(unknown))/float(len(tokens)) * 100)\n",
    "               # 3. Subtract the number from step 2 from 100.\n",
    "                    # 100 - (float(len(unknown))/float(len(tokens)) * 100)\n",
    "               # 4. Round the number from step 3 to 2 decimal places\n",
    "                    # round(100 - (float(len(unknown))/float(len(tokens)) * 100), 2)\n",
    "            \n",
    "        readability = round(100 - (float(len(unknown))/float(len(tokens)) * 100), 2)\n",
    "        \n",
    "        # If the list of unknown tokens is empty (or equal to 0), then readability is 100!\n",
    "    else:\n",
    "        readability = 100\n",
    "    \n",
    "    # Let's create a record of the readability information \n",
    "    # for this page that we'll add to the dataframe. \n",
    "    # The following is a Python dictionary, another way of \n",
    "    # storing data. Each word or phrase to the left of the : is a\n",
    "    # \"key\" -- think of it as a column header. Each piece of \n",
    "    # information to the right is a \"value\" -- information \n",
    "    # written in a single cell below each header. \n",
    "\n",
    "    df2 = pd.DataFrame({\n",
    "            \"file_name\" : txt_file.as_posix(),\n",
    "            \"token_count\" : len(tokens),\n",
    "            \"unknown_count\" : len(unknown),\n",
    "            \"readability\" : readability,\n",
    "            \"unknown_words\" : [unknown],\n",
    "            \"text\" : ocrText\n",
    "            })\n",
    "\n",
    "    df = pd.concat([df, df2])\n",
    "\n",
    "    # This statement lets us know if a page has been successfully \n",
    "    # checked for readability.\n",
    "    print(txt_file, \"checked for readability.\")\n",
    "    \n",
    "# This time, instead of creating individual .txt files for each page,\n",
    "# we're going to save all of the OCR'ed text and readability \n",
    "# information to a single .csv (\"comma separated value\") file. \n",
    "# We can view this file format as a table. Having everything stored \n",
    "# like this will help us with clean up and future analysis.\n",
    "df.to_csv('spellcheck_data.csv', header=True, index=False, sep=',')\n",
    "\n",
    "# We have the data stored in a file now, but we can also \n",
    "# preview it here:\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30da9866-7585-4a98-973a-601c3c991bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_words = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0', 'def quels', 'na ture', 'fem blables', 'mou rir', 'pro phetes', 'sor ciere', 'sor cier', 'sor ciers',\n",
    "                'plu-', 'def-', 'na-', 'fem-', 'mou-', 'pro-', 'sor-', 'sor-', 'fieurs', 'quels', 'ture', 'blables', 'rir', 'phetes', 'ciere', 'cier',\n",
    "                 'di able', 'dia', 'ble', 'empe-', 'reur', 'chan-', 'gement', 'expe-', 'rience', 'natu-', 'relle', 'ordi-', 'naire',\n",
    "                'imagi-', 'ner']\n",
    "known_words = ['', '', '', '', '', '', '', '', '', '', 'desquels', 'nature', 'femblables', 'mourir', 'prophetes', 'sorciere', 'sorcier', 'sorciers', 'plufieurs',\n",
    "              'defquels', 'nature', 'femblables', 'mourir', 'prophetes', 'sorciere', 'sorcier', '', '', '', '', '', '', '', '', 'diable', 'diable', '', 'empereur', '',\n",
    "               '', 'changement', '', 'experience', '', 'naturelle', 'ordinaire', '', 'imaginer', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c060d7ac-da17-45e2-9057-81654275a420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All instances of 1 replaced with .\n",
      "All instances of 2 replaced with .\n",
      "All instances of 3 replaced with .\n",
      "All instances of 4 replaced with .\n",
      "All instances of 5 replaced with .\n",
      "All instances of 6 replaced with .\n",
      "All instances of 7 replaced with .\n",
      "All instances of 8 replaced with .\n",
      "All instances of 9 replaced with .\n",
      "All instances of 0 replaced with .\n",
      "All instances of def quels replaced with desquels.\n",
      "All instances of na ture replaced with nature.\n",
      "All instances of fem blables replaced with femblables.\n",
      "All instances of mou rir replaced with mourir.\n",
      "All instances of pro phetes replaced with prophetes.\n",
      "All instances of sor ciere replaced with sorciere.\n",
      "All instances of sor cier replaced with sorcier.\n",
      "All instances of sor ciers replaced with sorciers.\n",
      "All instances of plu- replaced with plufieurs.\n",
      "All instances of def- replaced with defquels.\n",
      "All instances of na- replaced with nature.\n",
      "All instances of fem- replaced with femblables.\n",
      "All instances of mou- replaced with mourir.\n",
      "All instances of pro- replaced with prophetes.\n",
      "All instances of sor- replaced with sorciere.\n",
      "All instances of sor- replaced with sorcier.\n",
      "All instances of fieurs replaced with .\n",
      "All instances of quels replaced with .\n",
      "All instances of ture replaced with .\n",
      "All instances of blables replaced with .\n",
      "All instances of rir replaced with .\n",
      "All instances of phetes replaced with .\n",
      "All instances of ciere replaced with .\n",
      "All instances of cier replaced with .\n",
      "All instances of di able replaced with diable.\n",
      "All instances of dia replaced with diable.\n",
      "All instances of ble replaced with .\n",
      "All instances of empe- replaced with empereur.\n",
      "All instances of reur replaced with .\n",
      "All instances of chan- replaced with .\n",
      "All instances of gement replaced with changement.\n",
      "All instances of expe- replaced with .\n",
      "All instances of rience replaced with experience.\n",
      "All instances of natu- replaced with .\n",
      "All instances of relle replaced with naturelle.\n",
      "All instances of ordi- replaced with ordinaire.\n",
      "All instances of naire replaced with .\n",
      "All instances of imagi- replaced with imaginer.\n",
      "All instances of ner replaced with .\n"
     ]
    }
   ],
   "source": [
    "# Identify the sample_output file path.\n",
    "# Remember that our readability output is also stored \n",
    "# in this file as a .csv. We don't want to change it, \n",
    "# so we'll use glob to look for only .txt files.\n",
    "outputfile = f'{outputfile_path.as_posix()}/{doc_name}_corrected.txt'\n",
    "\n",
    "# Apply the following loop to one file at a time in filePath.\n",
    "for file in texts_folder.glob('*.txt'):\n",
    "    \n",
    "    # Open a file in \"read\" (r) mode.\n",
    "    text = open(file, \"r\")\n",
    "    \n",
    "    # Read in the contents of that file.\n",
    "    word_correction = text.read()\n",
    "\n",
    "    word_correction = word_correction.lower()\n",
    "    \n",
    "    # Find instances of and unknown word and replace\n",
    "    # with a known word\n",
    "    for i in range(len(known_words)):\n",
    "          \n",
    "        unknown_word = unknown_words[i]\n",
    "    \n",
    "        known_word = known_words[i]\n",
    "\n",
    "        word_correction = word_correction.replace(unknown_word, known_word)\n",
    "\n",
    "        print(\"All instances of \" + unknown_word + \" replaced with \" + known_word + \".\")    \n",
    " \n",
    "    # Reopen the file in \"write\" (w) mode.\n",
    "    file = open(outputfile, \"w\")\n",
    "    \n",
    "    # Add the changed word into the reopened file.\n",
    "    file.write(word_correction)\n",
    "    \n",
    "    # Close the file.\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e7ee69e-4202-404d-92c1-7d9f7b61ab79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spellchecked Démonomanie Repair_corrected.txt. Readability = 84.68\n"
     ]
    }
   ],
   "source": [
    "# Process each .txt file in the folder\n",
    "for filename in os.listdir(outputpath):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(outputpath, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text_data = file.read()\n",
    "\n",
    "        text_data = text_data.replace(\"-\\n\",\"\")\n",
    "    \n",
    "        words = wordpunct_tokenize(text_data)\n",
    "    \n",
    "        misspelled = spell.unknown(words)\n",
    "\n",
    "        if len(misspelled) != 0:\n",
    "                                \n",
    "            readability = round(100 - (float(len(misspelled))/float(len(words)) * 100), 2)\n",
    "        \n",
    "        # If the list of unknown tokens is empty (or equal to 0), then readability is 100!\n",
    "        else:\n",
    "            readability = 100\n",
    "\n",
    "        # Count the frequency of each misspelled word\n",
    "        word_counts = Counter(misspelled)\n",
    "\n",
    "        # SCreate a DataFrame and sort by frequency\n",
    "        misspelled_df = pd.DataFrame(word_counts.items(), columns=['word', 'count'])\n",
    "        misspelled_df = misspelled_df.sort_values(by='count', ascending=False)\n",
    "\n",
    "        # Save to a CSV file named after the .txt file\n",
    "        csv_filename = os.path.splitext(filename)[0] + '_spellechecker.csv'\n",
    "        misspelled_df.to_csv(os.path.join(outputpath, csv_filename), index=False)\n",
    "        \n",
    "        print(f'Spellchecked {filename}. Readability = {readability}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

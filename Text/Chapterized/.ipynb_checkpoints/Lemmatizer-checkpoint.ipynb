{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "230a299f-681f-4ee2-a5df-a37c6d009975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "\n",
    "# Initialize the stemmer and lemmatizer for French\n",
    "stemmer = SnowballStemmer(language='french')\n",
    "lemmatizer = FrenchLefffLemmatizer()\n",
    "\n",
    "# Define custom stemming/lemmatization rules sorted alphabetically with five entries per line\n",
    "CUSTOM_RULES = {\n",
    "    \"abfolu\": \"absolu\", \"accuf\": \"accus\", \"aduient\": \"advient\", \"aduis\": \"advis\", \"alor\": \"alors\",\n",
    "    \"ariftot\": \"aristot\", \"aufquel\": \"ausquel\", \"auantag\": \"avantag\", \"auon\": \"avon\", \"auoyent\": \"avoyent\",\n",
    "    \"beft\": \"best\", \"cai\": \"cayer\", \"ceft\": \"cest\", \"cftat\": \"estat\",\n",
    "    \"comiffion\": \"commission\", \"commiffion\": \"commission\", \"commiflair\": \"commissair\", \"commiflion\": \"commission\",\n",
    "    \"confeff\": \"confess\", \"conful\": \"consul\", \"coft\": \"cost\", \"costé\": \"côté\", \"defquel\": \"desquel\",\n",
    "    \"dieux\": \"dieu\", \"déput\": \"deput\", \"député\": \"deput\", \"depuré\": \"deput\", \"depurez\": \"deput\",\n",
    "    \"deuant\": \"devant\", \"desloix\": \"des loi\", \"deteft\": \"detest\", \"difent\": \"disent\",\n",
    "    \"difoit\": \"disoit\", \"diuif\": \"divis\", \"diuin\": \"divin\", \"droi\": \"doit\", \"droict\": \"droit\",\n",
    "    \"edit\": \"édict\", \"édictz\": \"édict\", \"eftant\": \"estant\", \"efté\": \"esté\", \"eftim\": \"estim\",\n",
    "    \"eftoyent\": \"estoyent\", \"efcript\": \"escript\", \"efpaign\": \"espaigne\", \"efpec\": \"espec\", \"efprit\": \"esprit\",\n",
    "    \"empefch\": \"empesch\", \"enfans\": \"enfant\", \"enfembl\": \"ensembl\", \"ensan\": \"enfant\", \"ensans\": \"enfant\",\n",
    "    \"enuer\": \"enver\", \"esglis\": \"églis\", \"espic\": \"épice\", \"estar\": \"estat\", \"estatz\": \"estat\",\n",
    "    \"état\": \"estat\", \"euft\": \"eust\", \"fag\": \"sag\", \"faif\": \"sais\", \"faifoit\": \"faisoit\",\n",
    "    \"facrific\": \"sacrific\", \"feliqu\": \"felicit\", \"fembl\": \"sembl\", \"femblabl\": \"semblabl\", \"fecond\": \"second\",\n",
    "    \"fept\": \"sept\", \"feulg\": \"seul\", \"felon\": \"selon\", \"fent\": \"sent\", \"feroit\": \"seroit\",\n",
    "    \"fcauoir\": \"savoir\", \"fcigneur\": \"seigneur\", \"fçauoir\": \"savoir\", \"finon\": \"sinon\", \"fimpl\": \"simpl\",\n",
    "    \"fignif\": \"signif\", \"file\": \"fill\", \"fong\": \"song\", \"foyent\": \"soyent\", \"fuft\": \"fust\",\n",
    "    \"foudain\": \"soudain\", \"hebrieux\": \"hebrieu\", \"iug\": \"jug\", \"iufqu\": \"iusqu\", \"iust\": \"just\", \"iustic\": \"justic\",\n",
    "    \"iurifdiet\": \"iurisdict\", \"iurifdict\": \"iurisdict\", \"laloy\": \"la loi\", \"laiff\": \"laiss\", \"leroy\": \"le roy\",\n",
    "    \"lefquel\": \"lesquel\", \"lesloix\": \"les loi\", \"lifon\": \"lison\", \"liur\": \"livr\", \"loix\": \"loi\",\n",
    "    \"loy\": \"loi\", \"loyx\": \"loi\", \"maicft\": \"maiest\", \"maifon\": \"maison\", \"maiftr\": \"maistr\",\n",
    "    \"magiftrat\": \"magistrat\", \"magiftrats\": \"magistrat\", \"magiltrat\": \"magistrat\", \"mefine\": \"même\", \"mefines\": \"mêmes\", \"meím\": \"mesm\", \n",
    "    \"mesmoir\": \"memoir\", \"monftr\": \"monstr\", \"noftr\": \"nostr\", \"pai\": \"pay\", \"parlement\": \"parlement\", \n",
    "    \"parlements\": \"parlement\", \"penf\": \"pens\", \"peuuent\": \"peuvent\", \"pluficur\": \"plusieur\", \n",
    "    \"plutfoft\": \"plustost\", \"pouuoir\": \"pouvoir\", \"pouuoit\": \"pouvoit\", \"pourueu\": \"pourveu\", \n",
    "    \"prefent\": \"present\", \"prefqu\": \"prequ\", \"prerogatiu\": \"prerogativ\", \"prin c\": \"prince\", \n",
    "    \"prin ces\": \"princ\", \"pris\": \"prix\", \"priuileg\": \"privileg\", \"procez\": \"procès\", \"puiff\": \"puiss\",\n",
    "    \"puifle\": \"puiss\", \"puiffanc\": \"puissanc\", \"quc\": \"que\", \"queftion\": \"question\", \"raifon\": \"raison\",\n",
    "    \"reffort\": \"ressort\", \"repvbliqu\": \"republ\", \"républicque\": \"républ\", \"royaulm\": \"royaum\", \"royau me\": \"royaum\",\n",
    "    \"sorcicr\": \"sorcier\", \"sorciere\": \"sorci\", \"sorcieres\": \"sorci\", \"soubverain\": \"souverain\", \n",
    "    \"souverian\": \"souverain\", \"souverianet\": \"souverainet\", \"subject\": \"sujet\", \"subjectz\": \"sujet\",\n",
    "    \"toufiour\": \"tousiour\", \"trouu\": \"trouv\", \"vertus\": \"vertu\", \"viur\": \"vivr\", \"vifion\": \"vision\",\n",
    "    \"ftatut\": \"statut\"\n",
    "}\n",
    "\n",
    "\n",
    "def custom_stem_or_lemmatize(word, use_lemmatizer):\n",
    "    \"\"\"\n",
    "    Apply custom rules during stemming or lemmatizing on a single token.\n",
    "    \"\"\"\n",
    "    if word in CUSTOM_RULES:\n",
    "        return CUSTOM_RULES[word]\n",
    "    if use_lemmatizer:\n",
    "        lemmatized_word = lemmatizer.lemmatize(word)\n",
    "        return CUSTOM_RULES.get(lemmatized_word, lemmatized_word)\n",
    "    else:\n",
    "        stemmed_word = stemmer.stem(word)\n",
    "        return CUSTOM_RULES.get(stemmed_word, stemmed_word)\n",
    "\n",
    "def return_stem(text, use_lemmatizer=False):\n",
    "    \"\"\"\n",
    "    Replace multi-token sequences first, then process the text token by token.\n",
    "\n",
    "    Multi-token replacement works by scanning the entire text for any key in CUSTOM_RULES\n",
    "    that contains a space. Since these keys represent phrases that should be merged into a single token,\n",
    "    the code replaces each occurrence with its corresponding value (which does not contain the space).\n",
    "    This way, when the text is later split into tokens (using text.split()),\n",
    "    phrases like \"royau me\" (if present in CUSTOM_RULES) will be replaced by \"royaum\" and treated as one token.\n",
    "    \"\"\"\n",
    "    # Replace multi-token sequences before tokenizing.\n",
    "    for phrase, replacement in CUSTOM_RULES.items():\n",
    "        if \" \" in phrase:\n",
    "            text = text.replace(phrase, replacement)\n",
    "    tokens = text.split()\n",
    "    final_processed = [custom_stem_or_lemmatize(token, use_lemmatizer) for token in tokens]\n",
    "    return ' '.join(final_processed)\n",
    "\n",
    "def process_file(file_path, use_lemmatizer=True):\n",
    "    \"\"\"\n",
    "    Process the contents of a file using custom rules.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "    processed_content = return_stem(content, use_lemmatizer)\n",
    "    return processed_content\n",
    "\n",
    "def save_processed_content(original_file, processed_content, use_lemmatizer):\n",
    "    \"\"\"\n",
    "    Save processed content to a new file in a 'lemmatized' subdirectory.\n",
    "    \"\"\"\n",
    "    output_dir = os.path.join(os.getcwd(), 'lemmatized')\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    suffix = '_lemmatized.txt' if use_lemmatizer else '_stemmed.txt'\n",
    "    new_file_name = os.path.splitext(os.path.basename(original_file))[0] + suffix\n",
    "    new_file_path = os.path.join(output_dir, new_file_name)\n",
    "    with open(new_file_path, 'w', encoding='utf-8') as file:\n",
    "        file.write(processed_content)\n",
    "    print(f\"Processed content saved to {new_file_path}\")\n",
    "\n",
    "def select_subdirectory():\n",
    "    \"\"\"\n",
    "    Prompt the user to select a subdirectory or use the current directory.\n",
    "    \"\"\"\n",
    "    current_directory = os.getcwd()\n",
    "    subdirectories = sorted([d for d in os.listdir(current_directory) if os.path.isdir(d)])\n",
    "    if not subdirectories:\n",
    "        print(\"No subdirectories found.\")\n",
    "        return None\n",
    "    print(\"Available subdirectories:\")\n",
    "    for i, subdir in enumerate(subdirectories):\n",
    "        print(f\"{i + 1}. {subdir}\")\n",
    "    while True:\n",
    "        user_input = input(\"Select a subdirectory number for target files (leave empty for current directory): \").strip()\n",
    "        if not user_input:\n",
    "            return current_directory\n",
    "        try:\n",
    "            choice = int(user_input) - 1\n",
    "            if choice < 0 or choice >= len(subdirectories):\n",
    "                print(\"Invalid choice. Please enter a valid number.\")\n",
    "                continue\n",
    "            return os.path.join(current_directory, subdirectories[choice])\n",
    "        except ValueError:\n",
    "            print(\"Invalid input. Please enter a number.\")\n",
    "\n",
    "def select_files():\n",
    "    \"\"\"\n",
    "    List and allow the user to select text files from a directory.\n",
    "    \"\"\"\n",
    "    directory = select_subdirectory()\n",
    "    if directory is None:\n",
    "        directory = os.getcwd()\n",
    "    txt_files = sorted([f for f in os.listdir(directory) if f.endswith('.txt')])\n",
    "    if not txt_files:\n",
    "        print(\"No text files found in the selected directory.\")\n",
    "        return []\n",
    "    print(\"Available text files:\")\n",
    "    for i, file in enumerate(txt_files):\n",
    "        print(f\"{i + 1}. {file}\")\n",
    "    selected_files = []\n",
    "    while True:\n",
    "        user_input = input(\"Select file numbers (e.g., 1,3-5 or press Enter to select all): \").strip()\n",
    "        if not user_input:\n",
    "            selected_files = txt_files\n",
    "            break\n",
    "        try:\n",
    "            parts = user_input.split(',')\n",
    "            for part in parts:\n",
    "                if '-' in part:\n",
    "                    start, end = map(int, part.split('-'))\n",
    "                    selected_files.extend(txt_files[start - 1:end])\n",
    "                else:\n",
    "                    selected_files.append(txt_files[int(part) - 1])\n",
    "            break\n",
    "        except (ValueError, IndexError):\n",
    "            print(\"Invalid input. Please enter valid numbers or ranges.\")\n",
    "    selected_files = [os.path.join(directory, file) for file in selected_files]\n",
    "    return selected_files\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to handle file processing.\n",
    "    \"\"\"\n",
    "    file_paths = select_files()\n",
    "    if not file_paths:\n",
    "        return\n",
    "    use_lemmatizer = input(\"Use FrenchLefffLemmatizer? (y/n, default is n): \").strip().lower() == 'y'\n",
    "    for file_path in file_paths:\n",
    "        processed_content = process_file(file_path, use_lemmatizer)\n",
    "        save_processed_content(file_path, processed_content, use_lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ac3dc68-3960-4bed-abbc-f3ba61d602f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available subdirectories:\n",
      "1. .ipynb_checkpoints\n",
      "2. Démonomanie\n",
      "3. République\n",
      "4. Théatre\n",
      "5. lemmatized\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select a subdirectory number for target files (leave empty for current directory):  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available text files:\n",
      "1. Discours des raisons_corrected.txt\n",
      "2. Démonomanie III_corrected.txt\n",
      "3. Démonomanie II_corrected.txt\n",
      "4. Démonomanie IV_corrected.txt\n",
      "5. Démonomanie I_corrected.txt\n",
      "6. Démonomanie preface Repair_corrected.txt\n",
      "7. Harangue - Fontainebleau_corrected.txt\n",
      "8. Harangue - Orléans 2_corrected.txt\n",
      "9. Harangue - Orléans_corrected.txt\n",
      "10. Harangue - Poissy_corrected.txt\n",
      "11. Harangue - Rouen_corrected.txt\n",
      "12. Harangue - Saint Germain_corrected.txt\n",
      "13. Harangue - lit de justice_corrected.txt\n",
      "14. Harangue - ouverture de parlement_corrected.txt\n",
      "15. Harangue - parlement 2_corrected.txt\n",
      "16. Harangue - parlement 3_corrected.txt\n",
      "17. Harangue - parlement_corrected.txt\n",
      "18. Harangue - religion_corrected.txt\n",
      "19. Harangue - septembre_corrected.txt\n",
      "20. La réponse_corrected.txt\n",
      "21. Le paradoxe_corrected.txt\n",
      "22. Lettre_corrected.txt\n",
      "23. Lit de justice_corrected.txt\n",
      "24. Memoire - Namur_corrected.txt\n",
      "25. Memoire - le but_corrected.txt\n",
      "26. Memoire au roi_corrected.txt\n",
      "27. Memoires d'État Refuge_corrected.txt\n",
      "28. Memoires d'état_corrected.txt\n",
      "29. Recueil_corrected.txt\n",
      "30. Remonstrances - Royaume_corrected.txt\n",
      "31. Remonstrances - parlement_corrected.txt\n",
      "32. République III_corrected.txt\n",
      "33. République II_corrected.txt\n",
      "34. République IV_corrected.txt\n",
      "35. République I_corrected.txt\n",
      "36. République Preface_corrected.txt\n",
      "37. République VI_corrected.txt\n",
      "38. République V_corrected.txt\n",
      "39. Théatre III_corrected.txt\n",
      "40. Théatre II_corrected.txt\n",
      "41. Théatre IV_corrected.txt\n",
      "42. Théatre I_corrected.txt\n",
      "43. Théatre V_corrected.txt\n",
      "44. Théatre summary_corrected.txt\n",
      "45. Traite Justice VII_corrected.txt\n",
      "46. Traite Justice VI_corrected.txt\n",
      "47. Traite Justice V_corrected.txt\n",
      "48. Traité Justice III_corrected.txt\n",
      "49. Traité Justice II_corrected.txt\n",
      "50. Traité Justice IV_corrected.txt\n",
      "51. Traité Justice I_corrected.txt\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select file numbers (e.g., 1,3-5 or press Enter to select all):  \n",
      "Use FrenchLefffLemmatizer? (y/n, default is n):  \n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'CUSTOM_RULES' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 168\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    166\u001b[0m use_lemmatizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUse FrenchLefffLemmatizer? (y/n, default is n): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_path \u001b[38;5;129;01min\u001b[39;00m file_paths:\n\u001b[0;32m--> 168\u001b[0m     processed_content \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_lemmatizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    169\u001b[0m     save_processed_content(file_path, processed_content, use_lemmatizer)\n",
      "Cell \u001b[0;32mIn[1], line 84\u001b[0m, in \u001b[0;36mprocess_file\u001b[0;34m(file_path, use_lemmatizer)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(file_path, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[1;32m     83\u001b[0m     content \u001b[38;5;241m=\u001b[39m file\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m---> 84\u001b[0m processed_content \u001b[38;5;241m=\u001b[39m \u001b[43mreturn_stem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_lemmatizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m processed_content\n",
      "Cell \u001b[0;32mIn[1], line 71\u001b[0m, in \u001b[0;36mreturn_stem\u001b[0;34m(text, use_lemmatizer)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;124;03mReplace multi-token sequences first, then process the text token by token.\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[38;5;124;03mphrases like \"royau me\" (if present in CUSTOM_RULES) will be replaced by \"royaum\" and treated as one token.\u001b[39;00m\n\u001b[1;32m     69\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;66;03m# Replace multi-token sequences before tokenizing.\u001b[39;00m\n\u001b[0;32m---> 71\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m phrase, replacement \u001b[38;5;129;01min\u001b[39;00m \u001b[43mCUSTOM_RULES\u001b[49m\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01min\u001b[39;00m phrase:\n\u001b[1;32m     73\u001b[0m         text \u001b[38;5;241m=\u001b[39m text\u001b[38;5;241m.\u001b[39mreplace(phrase, replacement)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'CUSTOM_RULES' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

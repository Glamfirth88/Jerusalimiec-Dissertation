{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dae2df9b-3aa2-4b68-b1a1-c426be6a30bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select your stopwords CSV:\n",
      "1. lemmatized/stop_words.csv\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select reference text directory:\n",
      "1. lemmatized/10Topics\n",
      "2. lemmatized\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available files:\n",
      "1. Bodin_stemmed.txt\n",
      "2. L'Hospital_stemmed.txt\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select files (indices, ranges, or prefix text):  1-2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select target text directory:\n",
      "1. lemmatized/10Topics\n",
      "2. lemmatized\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter number:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available files:\n",
      "1. Bodin_stemmed.txt\n",
      "2. L'Hospital_stemmed.txt\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Select files (indices, ranges, or prefix text):  1-2\n",
      "Enter Fisher’s Exact alpha threshold (e.g. 0.05):  0.075\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 325\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✅ Topic modeling pipeline completed successfully!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    324\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m--> 325\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[1], line 292\u001b[0m, in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    289\u001b[0m alpha \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter Fisher’s Exact alpha threshold (e.g. 0.05): \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip())\n\u001b[1;32m    291\u001b[0m \u001b[38;5;66;03m# 5) Prepare filtered training documents for MALLET\u001b[39;00m\n\u001b[0;32m--> 292\u001b[0m training_docs, distributions \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_training_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtarget_files\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstopwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;66;03m# 6) Specify number of topics and words per topic\u001b[39;00m\n\u001b[1;32m    297\u001b[0m num_topics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28minput\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnter number of topics to generate: \u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mstrip())\n",
      "Cell \u001b[0;32mIn[1], line 188\u001b[0m, in \u001b[0;36mprepare_training_data\u001b[0;34m(files, directory, stopwords, rate_dict, alpha)\u001b[0m\n\u001b[1;32m    183\u001b[0m tokens \u001b[38;5;241m=\u001b[39m tokenize_file(path)\n\u001b[1;32m    184\u001b[0m freq \u001b[38;5;241m=\u001b[39m Counter(tokens)\n\u001b[1;32m    186\u001b[0m filtered \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    187\u001b[0m     w \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m tokens\n\u001b[0;32m--> 188\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m w \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m stopwords \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mget_fishers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mw\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfreq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrate_dict\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m<\u001b[39m alpha\n\u001b[1;32m    189\u001b[0m ]\n\u001b[1;32m    190\u001b[0m docs\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(filtered))\n\u001b[1;32m    191\u001b[0m distributions\u001b[38;5;241m.\u001b[39mappend(freq)\n",
      "Cell \u001b[0;32mIn[1], line 144\u001b[0m, in \u001b[0;36mget_fishers\u001b[0;34m(word, freq_dict, rate_dict, alternative)\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;124;03mPerform Fisher’s Exact Test on one token:\u001b[39;00m\n\u001b[1;32m    139\u001b[0m \u001b[38;5;124;03m    [[observed, total-observed],\u001b[39;00m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m     [expected, total-expected]]\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03mReturns the p-value.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    143\u001b[0m observed \u001b[38;5;241m=\u001b[39m freq_dict\u001b[38;5;241m.\u001b[39mget(word, \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 144\u001b[0m total \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mfreq_dict\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    145\u001b[0m expected \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mround\u001b[39m(rate_dict\u001b[38;5;241m.\u001b[39mget(word, \u001b[38;5;241m0\u001b[39m) \u001b[38;5;241m*\u001b[39m total)\n\u001b[1;32m    146\u001b[0m table \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    147\u001b[0m     [observed, total \u001b[38;5;241m-\u001b[39m observed],\n\u001b[1;32m    148\u001b[0m     [expected, total \u001b[38;5;241m-\u001b[39m expected]\n\u001b[1;32m    149\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Integrated script for MALLET-based topic modeling with per-file Fisher’s Exact\n",
    "filtering and stopword exclusion. Supports UTF-8 encoding for French accents.\n",
    "Outputs:\n",
    "  - Topic keys\n",
    "  - Per-document topic distributions\n",
    "  - Heatmap visualization (PDF + JPG)\n",
    "  - Excel workbook with:\n",
    "      • Topic tokens\n",
    "      • Token counts per document\n",
    "      • Topic probabilities per document\n",
    "  - Separate Excel workbook with top-document titles per topic\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import fisher_exact\n",
    "import little_mallet_wrapper\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "\n",
    "# Path to your MALLET binary (adjust if needed)\n",
    "path_to_mallet = os.path.expanduser(\"~/mallet-2.0.8/bin/mallet\")\n",
    "\n",
    "\n",
    "def tokenize_file(filepath):\n",
    "    \"\"\"Load a UTF-8 text file and split on whitespace (preserves accents).\"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read().split()\n",
    "\n",
    "\n",
    "def list_txt_files(directory):\n",
    "    \"\"\"Return a sorted list of all .txt filenames in a directory.\"\"\"\n",
    "    return sorted(fn for fn in os.listdir(directory) if fn.endswith(\".txt\"))\n",
    "\n",
    "\n",
    "def list_csv_files(directory):\n",
    "    \"\"\"\n",
    "    Recursively find all .csv files under a directory, skipping\n",
    "    any .ipynb_checkpoints folders.\n",
    "    \"\"\"\n",
    "    csv_paths = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        if \".ipynb_checkpoints\" in dirs:\n",
    "            dirs.remove(\".ipynb_checkpoints\")\n",
    "        for fn in files:\n",
    "            if fn.lower().endswith(\".csv\"):\n",
    "                csv_paths.append(os.path.join(root, fn))\n",
    "    return csv_paths\n",
    "\n",
    "\n",
    "def choose_directory(prompt):\n",
    "    \"\"\"\n",
    "    Display a numbered list of immediate subdirectories (plus current dir)\n",
    "    and return the full path selected by the user.\n",
    "    \"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    base = Path(cwd).name\n",
    "    subdirs = [\n",
    "        d for d in os.listdir(cwd)\n",
    "        if os.path.isdir(os.path.join(cwd, d)) and d != \".ipynb_checkpoints\"\n",
    "    ]\n",
    "    options = [(os.path.join(cwd, d), f\"{base}/{d}\") for d in subdirs]\n",
    "    options.append((cwd, base))\n",
    "\n",
    "    print(prompt)\n",
    "    for i, (_, label) in enumerate(options, start=1):\n",
    "        print(f\"{i}. {label}\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"Enter number: \").strip())\n",
    "            if 1 <= choice <= len(options):\n",
    "                return options[choice - 1][0]\n",
    "        except ValueError:\n",
    "            pass\n",
    "        print(\"Invalid choice, please try again.\")\n",
    "\n",
    "\n",
    "def choose_files(filenames):\n",
    "    \"\"\"\n",
    "    Let user pick one or more filenames by:\n",
    "      - indices (\"2\" or \"1,3\")\n",
    "      - index ranges (\"1-4\")\n",
    "      - prefix matching (\"report\")\n",
    "      - or the keyword \"all\" to select every file.\n",
    "    Returns a sorted, unique list of chosen filenames.\n",
    "    \"\"\"\n",
    "    print(\"Available files:\")\n",
    "    for i, fn in enumerate(filenames, start=1):\n",
    "        print(f\"{i}. {fn}\")\n",
    "    choice = input(\"Select files (indices, ranges, prefix, or 'all'): \").strip().lower()\n",
    "\n",
    "    if choice == \"all\":\n",
    "        return filenames.copy()\n",
    "\n",
    "    selected = []\n",
    "    for part in choice.split(\",\"):\n",
    "        part = part.strip()\n",
    "        if part == \"all\":\n",
    "            return filenames.copy()\n",
    "        if \"-\" in part:\n",
    "            a, b = map(int, part.split(\"-\"))\n",
    "            selected.extend(filenames[a - 1:b])\n",
    "        elif part.isdigit():\n",
    "            selected.append(filenames[int(part) - 1])\n",
    "        else:\n",
    "            selected.extend(fn for fn in filenames if fn.startswith(part))\n",
    "    return sorted(set(selected))\n",
    "\n",
    "\n",
    "def choose_csv_file(csv_paths):\n",
    "    \"\"\"\n",
    "    Prompt the user to select one CSV from a list of paths.\n",
    "    Returns the chosen filepath.\n",
    "    \"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    base = Path(cwd).name\n",
    "    print(\"Select your stopwords CSV:\")\n",
    "    for i, full in enumerate(csv_paths, start=1):\n",
    "        rel = os.path.relpath(full, cwd)\n",
    "        print(f\"{i}. {base}/{rel}\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"Enter number: \").strip())\n",
    "            if 1 <= choice <= len(csv_paths):\n",
    "                return csv_paths[choice - 1]\n",
    "        except ValueError:\n",
    "            pass\n",
    "        print(\"Invalid choice, please try again.\")\n",
    "\n",
    "\n",
    "def read_stopwords(filepath):\n",
    "    \"\"\"Load stopwords from a CSV, splitting on commas and trimming whitespace.\"\"\"\n",
    "    sw = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for row in csv.reader(f):\n",
    "            for cell in row:\n",
    "                sw.extend(cell.split(\",\"))\n",
    "    return [w.strip() for w in sw if w.strip()]\n",
    "\n",
    "\n",
    "def get_fishers(word, freq_dict, rate_dict, alternative=\"greater\"):\n",
    "    \"\"\"\n",
    "    Perform Fisher’s Exact Test on one token:\n",
    "        [[observed, total-observed],\n",
    "         [expected, total-expected]]\n",
    "    Returns the p-value.\n",
    "    \"\"\"\n",
    "    observed = freq_dict.get(word, 0)\n",
    "    total = sum(freq_dict.values())\n",
    "    expected = round(rate_dict.get(word, 0) * total)\n",
    "    table = [\n",
    "        [observed, total - observed],\n",
    "        [expected, total - expected]\n",
    "    ]\n",
    "    _, pval = fisher_exact(table, alternative=alternative)\n",
    "    return pval\n",
    "\n",
    "\n",
    "def calculate_rate_dictionary(rate_files, rate_dir):\n",
    "    \"\"\"\n",
    "    Build a background rate dictionary from reference documents.\n",
    "    Returns a mapping { token: relative_frequency }.\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    total_tokens = 0\n",
    "    for fn in rate_files:\n",
    "        tokens = tokenize_file(os.path.join(rate_dir, fn))\n",
    "        counter.update(tokens)\n",
    "        total_tokens += len(tokens)\n",
    "    return {tok: cnt / total_tokens for tok, cnt in counter.items()}\n",
    "\n",
    "\n",
    "def prepare_training_data(files, directory, stopwords, rate_dict, alpha):\n",
    "    \"\"\"\n",
    "    For each file:\n",
    "      1. Tokenize and count every token.\n",
    "      2. Exclude stopwords and tokens with Fisher p-value ≥ alpha.\n",
    "      3. Collect filtered document text and raw token counts.\n",
    "    Prints per-file progress with elapsed time.\n",
    "    Returns:\n",
    "      - docs: list of filtered document strings for MALLET input\n",
    "      - distributions: list of Counter objects of raw token counts\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    distributions = []\n",
    "    total = len(files)\n",
    "    overall_start = time.time()\n",
    "\n",
    "    for idx, fn in enumerate(files, start=1):\n",
    "        file_start = time.time()\n",
    "        print(f\"[{idx}/{total}] ⏳ Processing '{fn}'... \", end=\"\", flush=True)\n",
    "\n",
    "        path = os.path.join(directory, fn)\n",
    "        tokens = tokenize_file(path)\n",
    "        freq = Counter(tokens)\n",
    "\n",
    "        filtered = [\n",
    "            w for w in tokens\n",
    "            if w not in stopwords and get_fishers(w, freq, rate_dict) < alpha\n",
    "        ]\n",
    "        docs.append(\" \".join(filtered))\n",
    "        distributions.append(freq)\n",
    "\n",
    "        elapsed = time.time() - file_start\n",
    "        kept = len(filtered)\n",
    "        before = len(tokens)\n",
    "        pct = (kept / before * 100) if before else 0\n",
    "        print(f\"done in {elapsed:.1f}s – kept {kept}/{before} tokens ({pct:.1f}%).\")\n",
    "\n",
    "    total_elapsed = time.time() - overall_start\n",
    "    print(f\"[Done] Prepared {total} documents in {total_elapsed:.1f}s.\\n\")\n",
    "    return docs, distributions\n",
    "\n",
    "\n",
    "def train_topic_model(training_docs, num_topics, output_dir, num_top_words):\n",
    "    \"\"\"\n",
    "    Train a MALLET model via little_mallet_wrapper, specifying how many\n",
    "    top words MALLET should output per topic.\n",
    "    Returns:\n",
    "      - topics: list of token lists (topic keys)\n",
    "      - doc_topics: list of probability lists (per-document topic distributions)\n",
    "    \"\"\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    little_mallet_wrapper.quick_train_topic_model(\n",
    "        path_to_mallet,\n",
    "        output_dir,\n",
    "        num_topics,\n",
    "        training_docs,\n",
    "        num_top_words=num_top_words\n",
    "    )\n",
    "    key_file = f\"{output_dir}/mallet.topic_keys.{num_topics}\"\n",
    "    topics = little_mallet_wrapper.load_topic_keys(key_file)\n",
    "\n",
    "    dist_file = f\"{output_dir}/mallet.doc_topics.{num_topics}\"\n",
    "    doc_topics = little_mallet_wrapper.load_document_topics(dist_file)\n",
    "    return topics, doc_topics\n",
    "\n",
    "\n",
    "def save_results_to_excel(excel_path, topics, token_distributions, doc_topics, files):\n",
    "    \"\"\"\n",
    "    Create an Excel workbook with:\n",
    "      1) 'Topics' sheet: one row per topic (Topic#, top tokens...)\n",
    "      2) One sheet per document: raw token counts\n",
    "      3) 'DocTopicDist' sheet: topic probabilities per document\n",
    "    \"\"\"\n",
    "    wb = Workbook()\n",
    "    # -- Topics sheet --\n",
    "    ws0 = wb.active\n",
    "    ws0.title = \"Topics\"\n",
    "    for idx, topic in enumerate(topics):\n",
    "        ws0.append([f\"Topic {idx}\"] + topic)\n",
    "\n",
    "    # -- Token counts per document --\n",
    "    for fn, dist in zip(files, token_distributions):\n",
    "        sheet = wb.create_sheet(title=Path(fn).stem)\n",
    "        df = pd.DataFrame.from_dict(dist, orient=\"index\", columns=[\"count\"])\n",
    "        for row in dataframe_to_rows(df, index=True, header=True):\n",
    "            sheet.append(row)\n",
    "\n",
    "    # -- Document-topic probabilities --\n",
    "    ws3 = wb.create_sheet(title=\"DocTopicDist\")\n",
    "    header = [\"Document\"] + [f\"Topic{t}\" for t in range(len(topics))]\n",
    "    ws3.append(header)\n",
    "    for fn, probs in zip(files, doc_topics):\n",
    "        ws3.append([Path(fn).stem] + [round(p, 4) for p in probs])\n",
    "\n",
    "    wb.save(excel_path)\n",
    "\n",
    "\n",
    "def save_top_titles_excel(xlsx_path, topics, training_docs, doc_topics, doc_titles, n_docs):\n",
    "    \"\"\"\n",
    "    Save the top n_docs document titles per topic into an Excel file,\n",
    "    one sheet per topic named 'Topic{#}'.\n",
    "    \"\"\"\n",
    "    wb = Workbook()\n",
    "    for t_idx in range(len(topics)):\n",
    "        ws = wb.create_sheet(title=f\"Topic{t_idx}\")\n",
    "        # header\n",
    "        ws.append([\"Probability\", \"Document Title\"])\n",
    "        # fetch top docs\n",
    "        for prob, doc in little_mallet_wrapper.get_top_docs(\n",
    "            training_docs, doc_topics, t_idx, n=n_docs\n",
    "        ):\n",
    "            title = doc_titles.get(doc, Path(doc).stem)\n",
    "            ws.append([round(prob, 4), title])\n",
    "    wb.save(xlsx_path)\n",
    "\n",
    "\n",
    "def export_heatmap(files, doc_topics, topics, output_dir):\n",
    "    \"\"\"\n",
    "    Generate and save a topic-by-document heatmap:\n",
    "      - PDF at categories_by_topics.pdf\n",
    "      - JPG at heatmap.jpg\n",
    "    Row labels come from each filename (without extension).\n",
    "    \"\"\"\n",
    "    labels = [Path(fn).stem for fn in files]\n",
    "    pdf_out = os.path.join(output_dir, \"categories_by_topics.pdf\")\n",
    "\n",
    "    fig = little_mallet_wrapper.plot_categories_by_topics_heatmap(\n",
    "        labels,\n",
    "        doc_topics,\n",
    "        topics,\n",
    "        pdf_out,\n",
    "        target_labels=labels,\n",
    "        dim=(13, 9)\n",
    "    )\n",
    "    fig.savefig(os.path.join(output_dir, \"heatmap.jpg\"),\n",
    "                format=\"jpg\",\n",
    "                dpi=300)\n",
    "\n",
    "\n",
    "def main():\n",
    "    # 1) Choose and load stopwords CSV\n",
    "    stop_csv = choose_csv_file(list_csv_files(os.getcwd()))\n",
    "    stopwords = read_stopwords(stop_csv)\n",
    "\n",
    "    # 2) Build background rate dictionary\n",
    "    rate_dir = choose_directory(\"Select reference text directory:\")\n",
    "    rate_files = choose_files(list_txt_files(rate_dir))\n",
    "    rate_dict = calculate_rate_dictionary(rate_files, rate_dir)\n",
    "\n",
    "    # 3) Select target files\n",
    "    target_dir = choose_directory(\"Select target text directory:\")\n",
    "    target_files = choose_files(list_txt_files(target_dir))\n",
    "\n",
    "    # 4) Fisher’s Exact threshold\n",
    "    alpha = float(input(\"Enter Fisher’s Exact alpha threshold (e.g. 0.05): \").strip())\n",
    "\n",
    "    # 5) Prepare filtered training data\n",
    "    training_docs, token_distributions = prepare_training_data(\n",
    "        target_files, target_dir, stopwords, rate_dict, alpha\n",
    "    )\n",
    "\n",
    "    # 6) Number of topics & top-words\n",
    "    num_topics = int(input(\"Enter number of topics to generate: \").strip())\n",
    "    num_top_words = int(input(\n",
    "        \"Enter how many top tokens MALLET should output per topic: \").strip()\n",
    "    )\n",
    "\n",
    "    # 7) How many top documents per topic\n",
    "    n_top_docs = int(input(\"Enter number of top documents per topic: \").strip())\n",
    "\n",
    "    # 8) Output folder\n",
    "    out_sub = input(\"Enter name for output folder: \").strip()\n",
    "    output_dir = os.path.join(os.getcwd(), out_sub)\n",
    "\n",
    "    # 9) Train model and load topic keys + doc-topic distributions\n",
    "    topics, doc_topics = train_topic_model(\n",
    "        training_docs, num_topics, output_dir, num_top_words\n",
    "    )\n",
    "\n",
    "    # 10) Save combined results to Excel\n",
    "    excel_results = os.path.join(output_dir, \"topic_model_results.xlsx\")\n",
    "    save_results_to_excel(\n",
    "        excel_results, topics, token_distributions, doc_topics, target_files\n",
    "    )\n",
    "\n",
    "    # 11) Generate and save heatmap\n",
    "    export_heatmap(target_files, doc_topics, topics, output_dir)\n",
    "\n",
    "    # 12) Save top titles per topic to separate workbook\n",
    "    doc_titles = {doc: Path(fn).stem for doc, fn in zip(training_docs, target_files)}\n",
    "    top_titles_path = os.path.join(output_dir, \"top_titles.xlsx\")\n",
    "    save_top_titles_excel(\n",
    "        top_titles_path, topics, training_docs, doc_topics, doc_titles, n_top_docs\n",
    "    )\n",
    "\n",
    "    print(\"✅ Topic modeling pipeline completed successfully!\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

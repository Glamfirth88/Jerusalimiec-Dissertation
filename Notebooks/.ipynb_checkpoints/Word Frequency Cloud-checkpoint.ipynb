{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fead0a0e-a006-4976-aef9-22c15841b95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import glob\n",
    "# Import Counter()\n",
    "from collections import Counter\n",
    "\n",
    "# For making wordclouds\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93839d2a-f94b-44f3-809e-75bcda31188a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select a subfolder for gram counts file:\n",
      "1. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Bodin/Démonomanie Repair/Concatenated/tokenized\n",
      "2. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Bodin/Démonomanie Repair/Concatenated/final\n",
      "3. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Bodin/Démonomanie Repair/Concatenated/.ipynb_checkpoints\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of your choice:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select a .csv file as gram_csv:\n",
      "1. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Bodin/Démonomanie Repair/Concatenated/tokenized/Démonomanie Repair_corrected_bigram_counts.csv\n",
      "2. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Bodin/Démonomanie Repair/Concatenated/tokenized/Démonomanie Repair_sorted_counts.csv\n",
      "3. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Bodin/Démonomanie Repair/Concatenated/tokenized/Démonomanie Repair_corrected_collocation_counts.csv\n",
      "4. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Bodin/Démonomanie Repair/Concatenated/tokenized/Démonomanie Repair_corrected_trigram_counts.csv\n",
      "5. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Bodin/Démonomanie Repair/Concatenated/tokenized/Démonomanie Repair_corrected_trigram_collocation_counts.csv\n",
      "6. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Bodin/Démonomanie Repair/Concatenated/tokenized/Démonomanie Repair_corrected_unigram_counts.csv\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of your choice:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select a .csv file as stopword_csv:\n",
      "1. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Bodin/Démonomanie Repair/Concatenated/spellcheck_data.csv\n",
      "2. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Bodin/Démonomanie Repair/Concatenated/stop_words.csv\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of your choice:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select a subfolder for sorted_counts:\n",
      "1. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Bodin/Démonomanie Repair/Concatenated/tokenized\n",
      "2. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Bodin/Démonomanie Repair/Concatenated/final\n",
      "3. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Bodin/Démonomanie Repair/Concatenated/.ipynb_checkpoints\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of your choice:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gram_csv = '/home/lucas-jerusalimiec/Documents/OCR Text/Text/Bodin/Démonomanie Repair/Concatenated/tokenized/Démonomanie Repair_corrected_bigram_counts.csv'\n",
      "stopword_csv = '/home/lucas-jerusalimiec/Documents/OCR Text/Text/Bodin/Démonomanie Repair/Concatenated/stop_words.csv'\n",
      "sorted_counts = '/home/lucas-jerusalimiec/Documents/OCR Text/Text/Bodin/Démonomanie Repair/Concatenated/tokenized/Démonomanie Repair_sorted_counts.csv'\n"
     ]
    }
   ],
   "source": [
    "def prompt_user_to_select(prompt, options):\n",
    "    print(prompt)\n",
    "    for i, option in enumerate(options, start=1):\n",
    "        print(f\"{i}. {option}\")\n",
    "    choice = int(input(\"Enter the number of your choice: \")) - 1\n",
    "    return options[choice]\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Prompt user to select a subfolder in the current working directory\n",
    "subfolders = [f.path for f in os.scandir(current_directory) if f.is_dir()]\n",
    "selected_subfolder = prompt_user_to_select(\"Select a subfolder for gram counts file:\", subfolders)\n",
    "\n",
    "# Search for .csv files in the selected subfolder\n",
    "csv_files_in_subfolder = glob.glob(os.path.join(selected_subfolder, '*.csv'))\n",
    "gram_csv = prompt_user_to_select(\"Select a .csv file as gram_csv:\", csv_files_in_subfolder)\n",
    "\n",
    "# Search for .csv files in the current working directory\n",
    "csv_files_in_current_directory = glob.glob(os.path.join(current_directory, '*.csv'))\n",
    "stopword_csv = prompt_user_to_select(\"Select a .csv file as stopword_csv:\", csv_files_in_current_directory)\n",
    "\n",
    "# Prompt user to select a subfolder for sorted_counts\n",
    "sorted_counts_subfolder = prompt_user_to_select(\"Select a subfolder for sorted_counts:\", subfolders)\n",
    "\n",
    "# Extract the filename from the gram_csv\n",
    "gram_csv_filename = os.path.basename(gram_csv)\n",
    "\n",
    "# Set the sorted_counts variable\n",
    "sorted_counts = os.path.join(sorted_counts_subfolder, f\"{gram_csv_filename }_sorted_counts.csv\")\n",
    "\n",
    "print(f\"gram_csv = '{gram_csv}'\")\n",
    "print(f\"stopword_csv = '{stopword_csv}'\")\n",
    "print(f\"sorted_counts = '{sorted_counts}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b6c7a518-229c-40f3-ac16-d9cb1f6b8104",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "\n",
    "with open(stopword_csv, mode='r') as f:\n",
    "    stop_words = list(csv.reader(f))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c063e574-ba86-446e-80aa-7a042d45ce19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize word frequency dictionary\n",
    "#word_frequency = defaultdict(int)\n",
    "word_frequency = Counter()\n",
    "\n",
    "with open(gram_csv, mode='r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)  # Skip the header row\n",
    "    for row in reader:\n",
    "        token, count = row[0], int(row[1])\n",
    "        # Convert token to lowercase\n",
    "        token = token.lower()\n",
    "        # Split the token into words\n",
    "        words = token.split()\n",
    "        # Check if all words are alphanumeric and not stop words\n",
    "        if all(word.isalpha() and word not in stop_words for word in words):\n",
    "            word_frequency[token] += count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e671fb3c-6af3-478a-901b-534e21c9a033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "livre second         45\n",
      "livre premier        40\n",
      "loy dieu             35\n",
      "malins efprits       27\n",
      "cas pareil           24\n",
      "livre qvatriesme     23\n",
      "comme dit            21\n",
      "comme di             20\n",
      "plus grand           19\n",
      "livre troisiesme     19\n",
      "quelque chofe        18\n",
      "malings efprits      17\n",
      "bien fort            16\n",
      "cy deflus            16\n",
      "faire mou            15\n",
      "peut faire           15\n",
      "cy apres             14\n",
      "dieu comme           14\n",
      "comme dict           14\n",
      "mil cinq             14\n",
      "encores plus         13\n",
      "faut donc            13\n",
      "cinq cens            13\n",
      "livre troisieme      13\n",
      "plus forte           12\n"
     ]
    }
   ],
   "source": [
    "for gram, count in word_frequency.most_common(25):\n",
    "    print(gram.ljust(20), count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "97e34442-79ae-44c7-9e32-944694712adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sorted_counts, mode = 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['gram', 'count'])\n",
    "    for gram, count in word_frequency.most_common():\n",
    "        writer.writerow([gram, count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "7149831e-ebbf-4571-9c2c-db355a4f8ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists.\n"
     ]
    }
   ],
   "source": [
    "### Download cloud image for our word cloud shape ###\n",
    "# It is not required to have a shape to create a word cloud\n",
    "# Define the download URL and the local file path\n",
    "download_url = 'https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/sample_cloud.png'\n",
    "local_file_path = './tokenized/sample_cloud.png'\n",
    "\n",
    "# Check if the file already exists\n",
    "if not os.path.exists(local_file_path):\n",
    "    urllib.request.urlretrieve(download_url, local_file_path)\n",
    "    print('Cloud shape downloaded.')\n",
    "else:\n",
    "    print('File already exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "78db8562-700c-4255-8c63-19e29b683022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wordcloud from our data\n",
    "\n",
    "#Word Cloud documentation https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html\n",
    "\n",
    "# Adding a mask shape of a cloud to your word cloud\n",
    "# By default, the shape will be a rectangle\n",
    "# You can specify any shape you like based on an image file\n",
    "cloud_mask = np.array(Image.open('./tokenized/sample_cloud.png')) # Specifies the location of the mask shape\n",
    "cloud_mask = np.where(cloud_mask > 3, 255, cloud_mask) # this line will take all values greater than 3 and make them 255 (white)\n",
    "\n",
    "### Specify word cloud details\n",
    "wordcloud = WordCloud(\n",
    "    width = 800, # Change the pixel width of the image if blurry\n",
    "    height = 600, # Change the pixel height of the image if blurry\n",
    "    background_color = \"white\", # Change the background color\n",
    "    colormap = 'viridis', # The colors of the words, see https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "    max_words = 150, # Change the max number of words shown\n",
    "    min_font_size = 4, # Do not show small text\n",
    "    \n",
    "    # Add a shape and outline (known as a mask) to your wordcloud\n",
    "    contour_color = 'blue', # The outline color of your mask shape\n",
    "    mask = cloud_mask, # \n",
    "    contour_width = 1\n",
    ").generate_from_frequencies(word_frequency)\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (20,20) # Change the image size displayed\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "#plt.show()\n",
    "\n",
    "plt.savefig(f'./tokenized/{gram_csv_filename }_wordcloud.png', format='png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Close the plot window after saving the image\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dcd07665-38bc-4fed-8621-4a04acb65976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import glob, a module that helps with file management.\n",
    "import glob\n",
    "import pandas as pd\n",
    "from spellchecker import SpellChecker\n",
    "from pathlib import Path\n",
    "from nltk import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a8251a74-964b-47ac-9752-9d785cb06cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spellchecker dictionary.\n",
    "# Replace the language attribute with another 2 letter code\n",
    "# to select another language. Options are: English - ‘en’, Spanish - ‘es’,\n",
    "# French - ‘fr’, Portuguese - ‘pt’, German - ‘de’, Russian - ‘ru’.\n",
    "\n",
    "spell = SpellChecker(language='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d411299b-c6ea-4ac9-9bc5-97628f5fcff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output/Démonomanie I.1.txt checked for readability.\n",
      "output/.ipynb_checkpoints/Démonomanie I.1-checkpoint.txt checked for readability.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>token_count</th>\n",
       "      <th>unknown_count</th>\n",
       "      <th>readability</th>\n",
       "      <th>unknown_words</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>placeholder</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>placeholder</td>\n",
       "      <td>placeholder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>output/Démonomanie I.1.txt</td>\n",
       "      <td>2518</td>\n",
       "      <td>903</td>\n",
       "      <td>64.14</td>\n",
       "      <td>{ip, parleau, oïes, ption, mefprife, helas, en...</td>\n",
       "      <td>\\n\\n         à p cS LV - : { - 2 e. / * 4 \\nz ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>output/.ipynb_checkpoints/Démonomanie I.1-chec...</td>\n",
       "      <td>2518</td>\n",
       "      <td>903</td>\n",
       "      <td>64.14</td>\n",
       "      <td>{ip, parleau, oïes, ption, mefprife, helas, en...</td>\n",
       "      <td>\\n\\n         à p cS LV - : { - 2 e. / * 4 \\nz ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_name  token_count  \\\n",
       "0                                        placeholder            0   \n",
       "0                         output/Démonomanie I.1.txt         2518   \n",
       "0  output/.ipynb_checkpoints/Démonomanie I.1-chec...         2518   \n",
       "\n",
       "   unknown_count  readability  \\\n",
       "0              0         0.00   \n",
       "0            903        64.14   \n",
       "0            903        64.14   \n",
       "\n",
       "                                       unknown_words  \\\n",
       "0                                        placeholder   \n",
       "0  {ip, parleau, oïes, ption, mefprife, helas, en...   \n",
       "0  {ip, parleau, oïes, ption, mefprife, helas, en...   \n",
       "\n",
       "                                                text  \n",
       "0                                        placeholder  \n",
       "0  \\n\\n         à p cS LV - : { - 2 e. / * 4 \\nz ...  \n",
       "0  \\n\\n         à p cS LV - : { - 2 e. / * 4 \\nz ...  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Dictionary Test a Folder of .txt Files ###\n",
    "\n",
    "# We'll use Pandas to create a dataframe (a table) that can hold \n",
    "# information about an OCR'ed page and display it in a tabular format.\n",
    "# This dataframe will start out empty with only its column headers \n",
    "# defined. We'll add information to it one page at a time. So each\n",
    "# row will represent 1 page.\n",
    "\n",
    "placeholder_values = ['placeholder', 0, 0, 0, 'placeholder', 'placeholder']\n",
    "\n",
    "df = pd.DataFrame([placeholder_values], columns=[\"file_name\",\"token_count\",\"unknown_count\",\"readability\",\"unknown_words\",\"text\"])\n",
    "\n",
    "# Set the folder for the input images\n",
    "texts_folder = Path('./output/')\n",
    "\n",
    "for txt_file in texts_folder.rglob('*.txt'):\n",
    "    \n",
    "    # Open each text file and read text into `ocrText`\n",
    "    with open(txt_file, 'r') as inputFile:\n",
    "        ocrText = inputFile.read()\n",
    "        \n",
    "    # Join hyphenated words that are split between lines by \n",
    "    # looking for a hyphen followed by a newline character: \"-\\n\"\n",
    "    # \"\\n\" is an \"escape character\" and represents the \n",
    "    # \"newline,\" a character that is usually invisible \n",
    "    # to human readers but that computers use to mark the \n",
    "    # end/beginning of a line. Each time you press the \n",
    "    # Enter/Return key on your keyboard, an invisible \"\\n\" \n",
    "    # is created to mark the beginning of a new line.\n",
    "    ocrText = ocrText.replace(\"-\\n\",\"\")\n",
    "    \n",
    "    # First, we'll use NLTK to \"tokenize\" text. \n",
    "            # \"Tokenize\" here means to take a page of our OCR'ed text,\n",
    "            # which Python is currently reading as one big glob of data,\n",
    "            # and separate each word out so that it can be read as an\n",
    "            # individual piece of data within a larger data structure \n",
    "            # (a list). This process also removes punctuation.\n",
    "    tokens = word_tokenize(ocrText)\n",
    "    \n",
    "    # Lowercase all tokens\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    \n",
    "    # Now we can get all of the words that don't match the \n",
    "    # spellchecker dictionary or our list of place names--\n",
    "    # these are the potential spelling errors.\n",
    "    unknown = spell.unknown(tokens)\n",
    "    \n",
    "    # Let's use a little math to find out how many potential \n",
    "    # spelling errors were identified. As part of this process, \n",
    "    # we'll create a \"readability\" score that will give us a \n",
    "    # percentage of how readable each file is--how much of the \n",
    "    # OCR'ed is \"correct.\"\n",
    "        \n",
    "    # If the list of unknown tokens (words) is greater than 0 \n",
    "    # (i.e. if the list is not empty):\n",
    "    if len(unknown) != 0:\n",
    "            \n",
    "               # Following order of operations, here's what's happening \n",
    "               # in the readability variable below:\n",
    "               # 1. Divide the number of unknown tokens (len(unknown)) \n",
    "                    # by the total number of tokens on the page\n",
    "                    # (len(tokens)). Use \"float\" to specify that Python\n",
    "                    # returns a decimal number:\n",
    "                        # (float(len(unknown))/float(len(tokens))\n",
    "               # 2. Multiply the number from step 1 by 100.\n",
    "                    # (float(len(unknown))/float(len(tokens)) * 100)\n",
    "               # 3. Subtract the number from step 2 from 100.\n",
    "                    # 100 - (float(len(unknown))/float(len(tokens)) * 100)\n",
    "               # 4. Round the number from step 3 to 2 decimal places\n",
    "                    # round(100 - (float(len(unknown))/float(len(tokens)) * 100), 2)\n",
    "            \n",
    "        readability = round(100 - (float(len(unknown))/float(len(tokens)) * 100), 2)\n",
    "        \n",
    "        # If the list of unknown tokens is empty (or equal to 0), then readability is 100!\n",
    "    else:\n",
    "        readability = 100\n",
    "    \n",
    "    # Let's create a record of the readability information \n",
    "    # for this page that we'll add to the dataframe. \n",
    "    # The following is a Python dictionary, another way of \n",
    "    # storing data. Each word or phrase to the left of the : is a\n",
    "    # \"key\" -- think of it as a column header. Each piece of \n",
    "    # information to the right is a \"value\" -- information \n",
    "    # written in a single cell below each header. \n",
    "\n",
    "    df2 = pd.DataFrame({\n",
    "            \"file_name\" : txt_file.as_posix(),\n",
    "            \"token_count\" : len(tokens),\n",
    "            \"unknown_count\" : len(unknown),\n",
    "            \"readability\" : readability,\n",
    "            \"unknown_words\" : [unknown],\n",
    "            \"text\" : ocrText\n",
    "            })\n",
    "\n",
    "    df = pd.concat([df, df2])\n",
    "\n",
    "    # This statement lets us know if a page has been successfully \n",
    "    # checked for readability.\n",
    "    print(txt_file, \"checked for readability.\")\n",
    "    \n",
    "# This time, instead of creating individual .txt files for each page,\n",
    "# we're going to save all of the OCR'ed text and readability \n",
    "# information to a single .csv (\"comma separated value\") file. \n",
    "# We can view this file format as a table. Having everything stored \n",
    "# like this will help us with clean up and future analysis.\n",
    "df.to_csv(f'{texts_folder}/spellcheck_data.csv', header=True, index=False, sep=',')\n",
    "\n",
    "# We have the data stored in a file now, but we can also \n",
    "# preview it here:\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef2b8e3-e9d9-4c48-af8c-3ac0d0b799b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the df variable in case we wish to run this script again\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c060d7ac-da17-45e2-9057-81654275a420",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify the sample_output file path.\n",
    "# Remember that our readability output is also stored \n",
    "# in this file as a .csv. We don't want to change it, \n",
    "# so we'll use glob to look for only .txt files.\n",
    "filePath = glob.glob(f\"{outDir}/*.txt\")\n",
    "\n",
    "outputfile_path = Path(\"./final\")\n",
    "\n",
    "outputfile_path.mkdir(exist_ok=True)\n",
    "\n",
    "outputfile = f'{outputfile_path.as_posix()}/{doc_name}_corrected.txt'\n",
    "\n",
    "# Apply the following loop to one file at a time in filePath.\n",
    "for file in filePath:\n",
    "    \n",
    "    # Open a file in \"read\" (r) mode.\n",
    "    text = open(file, \"r\")\n",
    "    \n",
    "    # Read in the contents of that file.\n",
    "    text = text.read()\n",
    "    \n",
    "    # Find instances of and unknown word and replace\n",
    "    # with a known word.\n",
    "    \n",
    "    unknown_word = \"|\"\n",
    "    \n",
    "    known_word = \"\"\n",
    "  \n",
    "    word_correction = text.replace(unknown_word, known_word)\n",
    "\n",
    "    print(\"All instances of \" + unknown_word + \" replaced with \" + known_word + \".\")\n",
    "\n",
    "    unknown_word = \"EE\"\n",
    "    \n",
    "    known_word = \"\"\n",
    "  \n",
    "    word_correction = word_correction.replace(unknown_word, known_word)\n",
    "\n",
    "    print(\"All instances of \" + unknown_word + \" replaced with \" + known_word + \".\")\n",
    "    \n",
    "    # Reopen the file in \"write\" (w) mode.\n",
    "    file = open(outputfile, \"w\")\n",
    "    \n",
    "    # Add the changed word into the reopened file.\n",
    "    file.write(word_correction)\n",
    "    \n",
    "    # Close the file.\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d990fc4-703b-4732-b3aa-3b5e0ead0dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Dictionary Test a Folder of .txt Files ###\n",
    "\n",
    "# We'll use Pandas to create a dataframe (a table) that can hold \n",
    "# information about an OCR'ed page and display it in a tabular format.\n",
    "# This dataframe will start out empty with only its column headers \n",
    "# defined. We'll add information to it one page at a time. So each\n",
    "# row will represent 1 page.\n",
    "\n",
    "placeholder_values = ['placeholder', 0, 0, 0, 'placeholder', 'placeholder']\n",
    "\n",
    "df = pd.DataFrame([placeholder_values], columns=[\"file_name\",\"token_count\",\"unknown_count\",\"readability\",\"unknown_words\",\"text\"])\n",
    "\n",
    "# Set the folder for the input images\n",
    "texts_folder = Path(\"./final\")\n",
    "\n",
    "for txt_file in texts_folder.rglob('*.txt'):\n",
    "    \n",
    "    # Open each text file and read text into `ocrText`\n",
    "    with open(txt_file, 'r') as inputFile:\n",
    "        ocrText = inputFile.read()\n",
    "        \n",
    "    # Join hyphenated words that are split between lines by \n",
    "    # looking for a hyphen followed by a newline character: \"-\\n\"\n",
    "    # \"\\n\" is an \"escape character\" and represents the \n",
    "    # \"newline,\" a character that is usually invisible \n",
    "    # to human readers but that computers use to mark the \n",
    "    # end/beginning of a line. Each time you press the \n",
    "    # Enter/Return key on your keyboard, an invisible \"\\n\" \n",
    "    # is created to mark the beginning of a new line.\n",
    "    ocrText = ocrText.replace(\"-\\n\",\"\")\n",
    "    \n",
    "    # First, we'll use NLTK to \"tokenize\" text. \n",
    "            # \"Tokenize\" here means to take a page of our OCR'ed text,\n",
    "            # which Python is currently reading as one big glob of data,\n",
    "            # and separate each word out so that it can be read as an\n",
    "            # individual piece of data within a larger data structure \n",
    "            # (a list). This process also removes punctuation.\n",
    "    tokens = word_tokenize(ocrText)\n",
    "    \n",
    "    # Lowercase all tokens\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    \n",
    "    # Now we can get all of the words that don't match the \n",
    "    # spellchecker dictionary or our list of place names--\n",
    "    # these are the potential spelling errors.\n",
    "    unknown = spell.unknown(tokens)\n",
    "    \n",
    "    # Let's use a little math to find out how many potential \n",
    "    # spelling errors were identified. As part of this process, \n",
    "    # we'll create a \"readability\" score that will give us a \n",
    "    # percentage of how readable each file is--how much of the \n",
    "    # OCR'ed is \"correct.\"\n",
    "        \n",
    "    # If the list of unknown tokens (words) is greater than 0 \n",
    "    # (i.e. if the list is not empty):\n",
    "    if len(unknown) != 0:\n",
    "            \n",
    "               # Following order of operations, here's what's happening \n",
    "               # in the readability variable below:\n",
    "               # 1. Divide the number of unknown tokens (len(unknown)) \n",
    "                    # by the total number of tokens on the page\n",
    "                    # (len(tokens)). Use \"float\" to specify that Python\n",
    "                    # returns a decimal number:\n",
    "                        # (float(len(unknown))/float(len(tokens))\n",
    "               # 2. Multiply the number from step 1 by 100.\n",
    "                    # (float(len(unknown))/float(len(tokens)) * 100)\n",
    "               # 3. Subtract the number from step 2 from 100.\n",
    "                    # 100 - (float(len(unknown))/float(len(tokens)) * 100)\n",
    "               # 4. Round the number from step 3 to 2 decimal places\n",
    "                    # round(100 - (float(len(unknown))/float(len(tokens)) * 100), 2)\n",
    "            \n",
    "        readability = round(100 - (float(len(unknown))/float(len(tokens)) * 100), 2)\n",
    "        \n",
    "        # If the list of unknown tokens is empty (or equal to 0), then readability is 100!\n",
    "    else:\n",
    "        readability = 100\n",
    "    \n",
    "    # Let's create a record of the readability information \n",
    "    # for this page that we'll add to the dataframe. \n",
    "    # The following is a Python dictionary, another way of \n",
    "    # storing data. Each word or phrase to the left of the : is a\n",
    "    # \"key\" -- think of it as a column header. Each piece of \n",
    "    # information to the right is a \"value\" -- information \n",
    "    # written in a single cell below each header. \n",
    "\n",
    "    df2 = pd.DataFrame({\n",
    "            \"file_name\" : txt_file.as_posix(),\n",
    "            \"token_count\" : len(tokens),\n",
    "            \"unknown_count\" : len(unknown),\n",
    "            \"readability\" : readability,\n",
    "            \"unknown_words\" : [unknown],\n",
    "            \"text\" : ocrText\n",
    "            })\n",
    "\n",
    "    df = pd.concat([df, df2])\n",
    "\n",
    "    # This statement lets us know if a page has been successfully \n",
    "    # checked for readability.\n",
    "    print(txt_file, \"checked for readability.\")\n",
    "    \n",
    "# This time, instead of creating individual .txt files for each page,\n",
    "# we're going to save all of the OCR'ed text and readability \n",
    "# information to a single .csv (\"comma separated value\") file. \n",
    "# We can view this file format as a table. Having everything stored \n",
    "# like this will help us with clean up and future analysis.\n",
    "df.to_csv(f'{texts_folder}/spellcheck_data2.csv', header=True, index=False, sep=',')\n",
    "\n",
    "# We have the data stored in a file now, but we can also \n",
    "# preview it here:\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999550c2-41c9-4d97-8231-f19619be5634",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the df variable in case we wish to run this script again\n",
    "del df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fead0a0e-a006-4976-aef9-22c15841b95e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import os\n",
    "\n",
    "import glob\n",
    "# Import Counter()\n",
    "from collections import Counter\n",
    "\n",
    "# For making wordclouds\n",
    "from wordcloud import WordCloud\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import urllib.request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b44e0030-fed8-421a-a962-a526b8af9da3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty Counter object called `word_frequency`\n",
    "word_frequency = Counter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93839d2a-f94b-44f3-809e-75bcda31188a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select a subfolder:\n",
      "1. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Démonomanie/Concatenated/tokenized\n",
      "2. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Démonomanie/Concatenated/final\n",
      "3. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Démonomanie/Concatenated/.ipynb_checkpoints\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of your choice:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select a .csv file as unigram_csv:\n",
      "1. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Démonomanie/Concatenated/tokenized/Démonomanie_corrected_trigram_counts.csv\n",
      "2. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Démonomanie/Concatenated/tokenized/Démonomanie_corrected_bigram_counts.csv\n",
      "3. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Démonomanie/Concatenated/tokenized/Démonomanie_corrected_collocation_counts.csv\n",
      "4. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Démonomanie/Concatenated/tokenized/Démonomanie_corrected_unigram_counts.csv\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of your choice:  4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select a .csv file as stopword_csv:\n",
      "1. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Démonomanie/Concatenated/spellcheck_data.csv\n",
      "2. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Démonomanie/Concatenated/stop_words.csv\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of your choice:  2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Select a subfolder for sorted_counts:\n",
      "1. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Démonomanie/Concatenated/tokenized\n",
      "2. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Démonomanie/Concatenated/final\n",
      "3. /home/lucas-jerusalimiec/Documents/OCR Text/Text/Démonomanie/Concatenated/.ipynb_checkpoints\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the number of your choice:  1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unigram_csv = '/home/lucas-jerusalimiec/Documents/OCR Text/Text/Démonomanie/Concatenated/tokenized/Démonomanie_corrected_unigram_counts.csv'\n",
      "stopword_csv = '/home/lucas-jerusalimiec/Documents/OCR Text/Text/Démonomanie/Concatenated/stop_words.csv'\n",
      "sorted_counts = '/home/lucas-jerusalimiec/Documents/OCR Text/Text/Démonomanie/Concatenated/tokenized/Démonomanie_sorted_counts.csv'\n"
     ]
    }
   ],
   "source": [
    "def prompt_user_to_select(prompt, options):\n",
    "    print(prompt)\n",
    "    for i, option in enumerate(options, start=1):\n",
    "        print(f\"{i}. {option}\")\n",
    "    choice = int(input(\"Enter the number of your choice: \")) - 1\n",
    "    return options[choice]\n",
    "\n",
    "# Get the current working directory\n",
    "current_directory = os.getcwd()\n",
    "\n",
    "# Prompt user to select a subfolder in the current working directory\n",
    "subfolders = [f.path for f in os.scandir(current_directory) if f.is_dir()]\n",
    "selected_subfolder = prompt_user_to_select(\"Select a subfolder:\", subfolders)\n",
    "\n",
    "# Search for .csv files in the selected subfolder\n",
    "csv_files_in_subfolder = glob.glob(os.path.join(selected_subfolder, '*.csv'))\n",
    "unigram_csv = prompt_user_to_select(\"Select a .csv file as unigram_csv:\", csv_files_in_subfolder)\n",
    "\n",
    "# Search for .csv files in the current working directory\n",
    "csv_files_in_current_directory = glob.glob(os.path.join(current_directory, '*.csv'))\n",
    "stopword_csv = prompt_user_to_select(\"Select a .csv file as stopword_csv:\", csv_files_in_current_directory)\n",
    "\n",
    "# Prompt user to select a subfolder for sorted_counts\n",
    "sorted_counts_subfolder = prompt_user_to_select(\"Select a subfolder for sorted_counts:\", subfolders)\n",
    "\n",
    "# Extract the prefix from the unigram_csv filename\n",
    "unigram_csv_filename = os.path.basename(unigram_csv)\n",
    "prefix = unigram_csv_filename.split('_')[0]\n",
    "\n",
    "# Set the sorted_counts variable\n",
    "sorted_counts = os.path.join(sorted_counts_subfolder, f\"{prefix}_sorted_counts.csv\")\n",
    "\n",
    "print(f\"unigram_csv = '{unigram_csv}'\")\n",
    "print(f\"stopword_csv = '{stopword_csv}'\")\n",
    "print(f\"sorted_counts = '{sorted_counts}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b6c7a518-229c-40f3-ac16-d9cb1f6b8104",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "\n",
    "with open(stopword_csv, mode='r') as f:\n",
    "    stop_words = list(csv.reader(f))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ae90ac50-7c08-4cc9-b285-99cab9b0a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(unigram_csv, mode='r') as file:\n",
    "    reader = csv.reader(file)\n",
    "    next(reader)\n",
    "    for row in reader:\n",
    "        token, count = row[0], int(row[1])\n",
    "        # Convert token to lowercase\n",
    "        token = token.lower()\n",
    "        # Check if the token is alphanumeric and not a stop word\n",
    "        if token.isalpha() and token not in stop_words:\n",
    "            word_frequency[token] += count    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e671fb3c-6af3-478a-901b-534e21c9a033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dieu                 676\n",
      "comme                480\n",
      "sors                 437\n",
      "plus                 426\n",
      "bien                 330\n",
      "sor                  244\n",
      "dia                  235\n",
      "autres               231\n",
      "in                   229\n",
      "faire                229\n",
      "di                   214\n",
      "livre                195\n",
      "peut                 194\n",
      "point                192\n",
      "tous                 182\n",
      "dela                 180\n",
      "faut                 173\n",
      "dit                  172\n",
      "sathan               168\n",
      "tout                 166\n",
      "quand                162\n",
      "mort                 161\n",
      "dire                 154\n",
      "sans                 146\n",
      "re                   146\n"
     ]
    }
   ],
   "source": [
    "for gram, count in word_frequency.most_common(25):\n",
    "    print(gram.ljust(20), count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97e34442-79ae-44c7-9e32-944694712adf",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(sorted_counts, mode = 'w') as f:\n",
    "    writer = csv.writer(f)\n",
    "    writer.writerow(['unigram', 'count'])\n",
    "    for gram, count in word_frequency.most_common():\n",
    "        writer.writerow([gram, count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7149831e-ebbf-4571-9c2c-db355a4f8ee6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloud shape downloaded.\n"
     ]
    }
   ],
   "source": [
    "### Download cloud image for our word cloud shape ###\n",
    "# It is not required to have a shape to create a word cloud\n",
    "download_url = 'https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/sample_cloud.png'\n",
    "urllib.request.urlretrieve(download_url, f'./tokenized/sample_cloud.png')\n",
    "print('Cloud shape downloaded.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "78db8562-700c-4255-8c63-19e29b683022",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a wordcloud from our data\n",
    "\n",
    "#Word Cloud documentation https://amueller.github.io/word_cloud/generated/wordcloud.WordCloud.html\n",
    "\n",
    "# Adding a mask shape of a cloud to your word cloud\n",
    "# By default, the shape will be a rectangle\n",
    "# You can specify any shape you like based on an image file\n",
    "cloud_mask = np.array(Image.open('./tokenized/sample_cloud.png')) # Specifies the location of the mask shape\n",
    "cloud_mask = np.where(cloud_mask > 3, 255, cloud_mask) # this line will take all values greater than 3 and make them 255 (white)\n",
    "\n",
    "### Specify word cloud details\n",
    "wordcloud = WordCloud(\n",
    "    width = 800, # Change the pixel width of the image if blurry\n",
    "    height = 600, # Change the pixel height of the image if blurry\n",
    "    background_color = \"white\", # Change the background color\n",
    "    colormap = 'viridis', # The colors of the words, see https://matplotlib.org/stable/tutorials/colors/colormaps.html\n",
    "    max_words = 150, # Change the max number of words shown\n",
    "    min_font_size = 4, # Do not show small text\n",
    "    \n",
    "    # Add a shape and outline (known as a mask) to your wordcloud\n",
    "    contour_color = 'blue', # The outline color of your mask shape\n",
    "    mask = cloud_mask, # \n",
    "    contour_width = 1\n",
    ").generate_from_frequencies(word_frequency)\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (20,20) # Change the image size displayed\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis(\"off\")\n",
    "#plt.show()\n",
    "\n",
    "plt.savefig('./tokenized/wordcloud.png', format='png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Close the plot window after saving the image\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

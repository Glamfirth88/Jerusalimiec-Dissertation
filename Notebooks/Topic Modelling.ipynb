{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483a7fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install MALLET 2.0.8RC3 from the official site\n",
    "!wget http://mallet.cs.umass.edu/dist/mallet-2.0.8RC3.zip -O ~/mallet-2.0.8RC3.zip\n",
    "\n",
    "# Unzip MALLET\n",
    "!unzip ~/mallet-2.0.8RC3.zip -d ~/\n",
    "!rm ~/mallet-2.0.8RC3.zip\n",
    "\n",
    "# Rename the folder for convenience\n",
    "!mv ~/mallet-2.0.8RC3 ~/mallet-2.0.8\n",
    "\n",
    "# Define the path to the \"mallet\" binary\n",
    "import os\n",
    "mallet_dir = os.path.expanduser('~/mallet-2.0.8')\n",
    "path_to_mallet = os.path.join(mallet_dir, 'bin', 'mallet')\n",
    "print(f\"Path to MALLET: {path_to_mallet}\")\n",
    "\n",
    "# Optionally check Java version\n",
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d4ef14",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install little_mallet_wrapper openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb7473b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Integrated script for MALLET-based topic modeling with per-file Fisher’s Exact\n",
    "filtering and stopword exclusion. UTF-8 friendly.\n",
    "Outputs:\n",
    "  - Topic keys\n",
    "  - Per-document topic distributions\n",
    "  - Excel workbook with:\n",
    "      • Topic tokens\n",
    "      • Token counts per document\n",
    "      • Topic probabilities per document\n",
    "  - Separate Excel workbook with top-document titles per topic\n",
    "\n",
    "Graphical output (heatmap & boxplots) is disabled in this pipeline.\n",
    "See plotting_helpers.py for on-demand plotting.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import time\n",
    "from pathlib import Path\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd\n",
    "from scipy.stats import fisher_exact\n",
    "import little_mallet_wrapper\n",
    "from openpyxl import Workbook\n",
    "from openpyxl.utils.dataframe import dataframe_to_rows\n",
    "import pickle\n",
    "\n",
    "# Change this path to point to the local MALLET binary in the current directory.\n",
    "# For example, place the mallet-2.0.8 folder in the same directory as this script.\n",
    "# Then use os.path.join(...) relative to the current working directory (codespace).\n",
    "mallet_dir = os.path.expanduser('~/mallet-2.0.8')\n",
    "path_to_mallet = os.path.join(mallet_dir, 'bin', 'mallet')\n",
    "\n",
    "# These will be set by main() and picked up by plotting_helpers.py\n",
    "OUTPUT_DIR = None\n",
    "NUM_TOPICS = None\n",
    "\n",
    "def tokenize_file(filepath):\n",
    "    \"\"\"Load a UTF-8 text file and split on whitespace (preserves accents).\"\"\"\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        return f.read().split()\n",
    "\n",
    "def list_txt_files(directory):\n",
    "    \"\"\"Return a sorted list of all .txt filenames in a directory.\"\"\"\n",
    "    return sorted(fn for fn in os.listdir(directory) if fn.endswith(\".txt\"))\n",
    "\n",
    "def list_csv_files(directory):\n",
    "    \"\"\"\n",
    "    Recursively find all .csv files under a directory, skipping\n",
    "    any .ipynb_checkpoints folders.\n",
    "    \"\"\"\n",
    "    csv_paths = []\n",
    "    for root, dirs, files in os.walk(directory):\n",
    "        if \".ipynb_checkpoints\" in dirs:\n",
    "            dirs.remove(\".ipynb_checkpoints\")\n",
    "        for fn in files:\n",
    "            if fn.lower().endswith(\".csv\"):\n",
    "                csv_paths.append(os.path.join(root, fn))\n",
    "    return csv_paths\n",
    "\n",
    "def choose_directory(prompt):\n",
    "    \"\"\"\n",
    "    Display a numbered list of immediate subdirectories (plus current dir)\n",
    "    and return the full path selected by the user.\n",
    "    \"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    base = Path(cwd).name\n",
    "    subdirs = [\n",
    "        d for d in os.listdir(cwd)\n",
    "        if os.path.isdir(os.path.join(cwd, d)) and d != \".ipynb_checkpoints\"\n",
    "    ]\n",
    "    options = [(os.path.join(cwd, d), f\"{base}/{d}\") for d in subdirs]\n",
    "    options.append((cwd, base))\n",
    "\n",
    "    print(prompt)\n",
    "    for i, (_, label) in enumerate(options, start=1):\n",
    "        print(f\"{i}. {label}\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"Enter number: \").strip())\n",
    "            if 1 <= choice <= len(options):\n",
    "                return options[choice - 1][0]\n",
    "        except ValueError:\n",
    "            pass\n",
    "        print(\"Invalid choice, please try again.\")\n",
    "\n",
    "def choose_files(filenames):\n",
    "    \"\"\"\n",
    "    Let user pick one or more filenames by:\n",
    "      - indices (\"2\" or \"1,3\")\n",
    "      - index ranges (\"1-4\")\n",
    "      - prefix matching (\"report\")\n",
    "      - or the keyword \"all\" to select every file.\n",
    "    Returns a sorted, unique list of chosen filenames.\n",
    "    \"\"\"\n",
    "    print(\"Available files:\")\n",
    "    for i, fn in enumerate(filenames, start=1):\n",
    "        print(f\"{i}. {fn}\")\n",
    "    choice = input(\"Select files (indices, ranges, prefix, or 'all'): \").strip().lower()\n",
    "\n",
    "    if choice == \"all\":\n",
    "        return filenames.copy()\n",
    "\n",
    "    selected = []\n",
    "    for part in choice.split(\",\"):\n",
    "        part = part.strip()\n",
    "        if part == \"all\":\n",
    "            return filenames.copy()\n",
    "        if \"-\" in part:\n",
    "            a, b = map(int, part.split(\"-\"))\n",
    "            selected.extend(filenames[a - 1:b])\n",
    "        elif part.isdigit():\n",
    "            selected.append(filenames[int(part) - 1])\n",
    "        else:\n",
    "            selected.extend(fn for fn in filenames if fn.startswith(part))\n",
    "    return sorted(set(selected))\n",
    "\n",
    "def choose_csv_file(csv_paths):\n",
    "    \"\"\"\n",
    "    Prompt the user to select one CSV from a list of paths.\n",
    "    Returns the chosen filepath.\n",
    "    \"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    base = Path(cwd).name\n",
    "    print(\"Select your stopwords CSV:\")\n",
    "    for i, full in enumerate(csv_paths, start=1):\n",
    "        rel = os.path.relpath(full, cwd)\n",
    "        print(f\"{i}. {base}/{rel}\")\n",
    "\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"Enter number: \").strip())\n",
    "            if 1 <= choice <= len(csv_paths):\n",
    "                return csv_paths[choice - 1]\n",
    "        except ValueError:\n",
    "            pass\n",
    "        print(\"Invalid choice, please try again.\")\n",
    "\n",
    "def read_stopwords(filepath):\n",
    "    \"\"\"Load stopwords from a CSV, splitting on commas and trimming whitespace.\"\"\"\n",
    "    sw = []\n",
    "    with open(filepath, \"r\", encoding=\"utf-8\") as f:\n",
    "        for row in csv.reader(f):\n",
    "            for cell in row:\n",
    "                sw.extend(cell.split(\",\"))\n",
    "    return [w.strip() for w in sw if w.strip()]\n",
    "\n",
    "def get_fishers(word, freq_dict, rate_dict, alternative=\"greater\"):\n",
    "    \"\"\"\n",
    "    Perform Fisher’s Exact Test on one token:\n",
    "        [[observed, total-observed],\n",
    "         [expected, total-expected]]\n",
    "    Returns the p-value.\n",
    "    \"\"\"\n",
    "    observed = freq_dict.get(word, 0)\n",
    "    total = sum(freq_dict.values())\n",
    "    expected = round(rate_dict.get(word, 0) * total)\n",
    "    table = [\n",
    "        [observed, total - observed],\n",
    "        [expected, total - expected]\n",
    "    ]\n",
    "    _, pval = fisher_exact(table, alternative=alternative)\n",
    "    return pval\n",
    "\n",
    "def calculate_rate_dictionary(rate_files, rate_dir):\n",
    "    \"\"\"\n",
    "    Build a background rate dictionary from reference documents.\n",
    "    Returns a mapping { token: relative_frequency }.\n",
    "    \"\"\"\n",
    "    counter = Counter()\n",
    "    total_tokens = 0\n",
    "    for fn in rate_files:\n",
    "        tokens = tokenize_file(os.path.join(rate_dir, fn))\n",
    "        counter.update(tokens)\n",
    "        total_tokens += len(tokens)\n",
    "    return {tok: cnt / total_tokens for tok, cnt in counter.items()}\n",
    "\n",
    "def prepare_training_data(files, directory, stopwords, rate_dict, alpha):\n",
    "    \"\"\"\n",
    "    For each file:\n",
    "      1. Tokenize and count every token.\n",
    "      2. Exclude stopwords and tokens with Fisher p-value ≥ alpha.\n",
    "      3. Collect filtered document text and raw token counts.\n",
    "    Prints per-file progress with elapsed time.\n",
    "    Returns:\n",
    "      - docs: list of filtered document strings for MALLET input\n",
    "      - distributions: list of Counter objects of raw token counts\n",
    "    \"\"\"\n",
    "    docs = []\n",
    "    distributions = []\n",
    "    total = len(files)\n",
    "    overall_start = time.time()\n",
    "\n",
    "    for idx, fn in enumerate(files, start=1):\n",
    "        file_start = time.time()\n",
    "        print(f\"[{idx}/{total}] ⏳ Processing '{fn}'... \")\n",
    "\n",
    "        path = os.path.join(directory, fn)\n",
    "        tokens = tokenize_file(path)\n",
    "        freq = Counter(tokens)\n",
    "\n",
    "        filtered = [\n",
    "            w for w in tokens\n",
    "            if w not in stopwords and get_fishers(w, freq, rate_dict) < alpha\n",
    "        ]\n",
    "        docs.append(\" \".join(filtered))\n",
    "        distributions.append(freq)\n",
    "\n",
    "        elapsed = time.time() - file_start\n",
    "        kept = len(filtered)\n",
    "        before = len(tokens)\n",
    "        pct = (kept / before * 100) if before else 0\n",
    "        print(f\"done in {elapsed:.1f}s – kept {kept}/{before} tokens ({pct:.1f}%).\")\n",
    "\n",
    "    total_elapsed = time.time() - overall_start\n",
    "    print(f\"[Done] Prepared {total} documents in {total_elapsed:.1f}s.\\n\")\n",
    "    return docs, distributions\n",
    "\n",
    "def train_topic_model(training_docs, num_topics, output_dir):\n",
    "    \"\"\"\n",
    "    Train a MALLET model via little_mallet_wrapper\n",
    "    Returns:\n",
    "      - topics: list of token lists (topic keys)\n",
    "      - doc_topics: list of probability lists (per-document topic distributions)\n",
    "    \"\"\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Make sure MALLET is installed and that path_to_mallet points to the local file in the codespace.\n",
    "    little_mallet_wrapper.quick_train_topic_model(\n",
    "        path_to_mallet,\n",
    "        output_dir,\n",
    "        num_topics,\n",
    "        training_docs\n",
    "    )\n",
    "\n",
    "    key_file = os.path.join(output_dir, f\"mallet.topic_keys.{num_topics}\")\n",
    "    topics = little_mallet_wrapper.load_topic_keys(key_file)\n",
    "\n",
    "    dist_file = os.path.join(output_dir, f\"mallet.topic_distributions.{num_topics}\")\n",
    "    doc_topics = little_mallet_wrapper.load_topic_distributions(dist_file)\n",
    "\n",
    "    return topics, doc_topics\n",
    "\n",
    "def save_results_to_excel(excel_path, topics, token_distributions, doc_topics, files):\n",
    "    \"\"\"\n",
    "    Create an Excel workbook with:\n",
    "      1) 'Topics' sheet: one row per topic (Topic#, top tokens...)\n",
    "      2) One sheet per document: raw token counts (removing '_stemmed' suffix)\n",
    "      3) 'DocTopicDist' sheet: topic probabilities per document\n",
    "    \"\"\"\n",
    "    wb = Workbook()\n",
    "    ws0 = wb.active\n",
    "    ws0.title = \"Topics\"\n",
    "    for idx, topic in enumerate(topics):\n",
    "        ws0.append([f\"Topic {idx}\"] + topic)\n",
    "\n",
    "    for fn, dist in zip(files, token_distributions):\n",
    "        sheet_name = Path(fn).stem.replace(\"_stemmed\", \"\")\n",
    "        sheet = wb.create_sheet(title=sheet_name)\n",
    "        df = pd.DataFrame.from_dict(dist, orient=\"index\", columns=[\"count\"])\n",
    "        for row in dataframe_to_rows(df, index=True, header=True):\n",
    "            sheet.append(row)\n",
    "\n",
    "    ws3 = wb.create_sheet(title=\"DocTopicDist\")\n",
    "    header = [\"Document\"] + [f\"Topic{t}\" for t in range(len(topics))]\n",
    "    ws3.append(header)\n",
    "    for fn, probs in zip(files, doc_topics):\n",
    "        doc_name = Path(fn).stem.replace(\"_stemmed\", \"\")\n",
    "        ws3.append([doc_name] + [round(p, 4) for p in probs])\n",
    "\n",
    "    wb.save(excel_path)\n",
    "\n",
    "def save_top_titles_excel(xlsx_path, topics, training_docs, doc_topics, doc_titles, n_docs):\n",
    "    \"\"\"\n",
    "    Save the top n_docs document titles per topic into an Excel file,\n",
    "    one sheet per topic named 'Topic{#}', and remove the default blank sheet.\n",
    "    \"\"\"\n",
    "    wb = Workbook()\n",
    "    default_sheet = wb.active\n",
    "    wb.remove(default_sheet)\n",
    "\n",
    "    for t_idx in range(len(topics)):\n",
    "        ws = wb.create_sheet(title=f\"Topic{t_idx}\")\n",
    "        ws.append([\"Probability\", \"Document Title\"])\n",
    "        for prob, doc in little_mallet_wrapper.get_top_docs(\n",
    "            training_docs, doc_topics, t_idx, n=n_docs\n",
    "        ):\n",
    "            title = doc_titles.get(doc, Path(doc).stem)\n",
    "            ws.append([round(prob, 4), title])\n",
    "\n",
    "    wb.save(xlsx_path)\n",
    "\n",
    "def input_float(prompt, min_val=None, max_val=None, default=None):\n",
    "    \"\"\"Prompt until the user enters a valid float (and optionally within range).\"\"\"\n",
    "    while True:\n",
    "        # Add default value to prompt if it exists\n",
    "        if default is not None:\n",
    "            prompt_with_default = f\"{prompt} [default: {default}] \"\n",
    "        else:\n",
    "            prompt_with_default = prompt\n",
    "        \n",
    "        resp = input(prompt_with_default).strip()\n",
    "        \n",
    "        # Return default if response is empty\n",
    "        if not resp and default is not None:\n",
    "            return default\n",
    "            \n",
    "        try:\n",
    "            val = float(resp)\n",
    "            if min_val is not None and val < min_val:\n",
    "                print(f\"Value must be at least {min_val}.\")\n",
    "                continue\n",
    "            if max_val is not None and val > max_val:\n",
    "                print(f\"Value must be at most {max_val}.\")\n",
    "                continue\n",
    "            return val\n",
    "        except ValueError:\n",
    "            print(\"Invalid input, please enter a valid number.\")\n",
    "\n",
    "def input_int(prompt, min_val=None, max_val=None):\n",
    "    \"\"\"Prompt until the user enters a valid integer (and optionally within range).\"\"\"\n",
    "    while True:\n",
    "        resp = input(prompt).strip()\n",
    "        try:\n",
    "            val = int(resp)\n",
    "            if min_val is not None and val < min_val:\n",
    "                print(f\"Value must be at least {min_val}.\")\n",
    "                continue\n",
    "            if max_val is not None and val > max_val:\n",
    "                print(f\"Value must be at most {max_val}.\")\n",
    "                continue\n",
    "            return val\n",
    "        except ValueError:\n",
    "            print(\"Invalid input, please enter a valid integer.\")\n",
    "\n",
    "def main():\n",
    "    global OUTPUT_DIR, NUM_TOPICS\n",
    "\n",
    "    # Prompt user: reuse preprocessed data or run full pipeline\n",
    "    reuse = input(\"Reuse preprocessed data? (y/n): \").strip().lower()\n",
    "    if reuse == 'y':\n",
    "        # Enumerate possible folders containing preprocessed data\n",
    "        cwd = os.getcwd()\n",
    "        # List all subdirectories in cwd that contain 'preprocessed_docs.pkl'\n",
    "        candidates = []\n",
    "        for d in os.listdir(cwd):\n",
    "            full_path = os.path.join(cwd, d)\n",
    "            if os.path.isdir(full_path):\n",
    "                pkl_path = os.path.join(full_path, \"preprocessed_docs.pkl\")\n",
    "                if os.path.exists(pkl_path):\n",
    "                    candidates.append(d)\n",
    "        if not candidates:\n",
    "            print(\"No folders with preprocessed data found in this directory.\")\n",
    "            return\n",
    "        print(\"Select a folder containing preprocessed data:\")\n",
    "        for i, d in enumerate(candidates, start=1):\n",
    "            print(f\"{i}. {d}\")\n",
    "        while True:\n",
    "            try:\n",
    "                choice = int(input(\"Enter number: \").strip())\n",
    "                if 1 <= choice <= len(candidates):\n",
    "                    pre_dir = candidates[choice - 1]\n",
    "                    break\n",
    "            except ValueError:\n",
    "                pass\n",
    "            print(\"Invalid choice, please try again.\")\n",
    "        OUTPUT_DIR = os.path.join(cwd, pre_dir)\n",
    "        with open(os.path.join(OUTPUT_DIR, \"preprocessed_docs.pkl\"), \"rb\") as f:\n",
    "            training_docs, token_distributions, target_files = pickle.load(f)\n",
    "        print(f\"Loaded preprocessed data from {OUTPUT_DIR}.\")\n",
    "    else:\n",
    "        # 1) Choose and load stopwords CSV\n",
    "        stop_csv = choose_csv_file(list_csv_files(os.getcwd()))\n",
    "        stopwords = read_stopwords(stop_csv)\n",
    "\n",
    "        # 2) Build background rate dictionary\n",
    "        rate_dir = choose_directory(\"Select reference text directory:\")\n",
    "        rate_files = choose_files(list_txt_files(rate_dir))\n",
    "        rate_dict = calculate_rate_dictionary(rate_files, rate_dir)\n",
    "\n",
    "        # 3) Select target files\n",
    "        target_dir = choose_directory(\"Select target text directory:\")\n",
    "        target_files = choose_files(list_txt_files(target_dir))\n",
    "\n",
    "        # 4) Output folder\n",
    "        while True:\n",
    "            out_sub = input(\"Enter name for output folder: \").strip()\n",
    "            if out_sub:\n",
    "                break\n",
    "            print(\"Output folder name cannot be empty.\")\n",
    "        OUTPUT_DIR = os.path.join(os.getcwd(), out_sub)\n",
    "        Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # 5) Fisher’s Exact threshold\n",
    "        alpha = input_float(\n",
    "            \"Enter Fisher’s Exact alpha threshold:\",\n",
    "            min_val=0.0, max_val=1.0, default=0.05\n",
    "        )\n",
    "\n",
    "        # 6) Prepare filtered training data\n",
    "        training_docs, token_distributions = prepare_training_data(\n",
    "            target_files, target_dir, stopwords, rate_dict, alpha\n",
    "        )\n",
    "\n",
    "        # 7) Save preprocessed data for reuse\n",
    "        with open(os.path.join(OUTPUT_DIR, \"preprocessed_docs.pkl\"), \"wb\") as f:\n",
    "            pickle.dump((training_docs, token_distributions, target_files), f)\n",
    "        print(f\"Preprocessed data saved to {OUTPUT_DIR}.\")\n",
    "\n",
    "    # Prompt for one or more numbers of topics\n",
    "    topic_str = input(\"Enter number(s) of topics to generate (comma-separated, e.g. 10,20,30): \").strip()\n",
    "    topic_nums = [int(x) for x in topic_str.split(\",\") if x.strip().isdigit()]\n",
    "    if not topic_nums:\n",
    "        print(\"No valid topic numbers entered. Exiting.\")\n",
    "        return\n",
    "\n",
    "    # Prompt for number of top documents per topic\n",
    "    n_top_docs = input_int(\"Enter number of top documents per topic: \", min_val=1)\n",
    "    for num_topics in topic_nums:\n",
    "        print(f\"\\n--- Training topic model with {num_topics} topics ---\")\n",
    "        NUM_TOPICS = num_topics\n",
    "        run_dir = os.path.join(OUTPUT_DIR, f\"topics_{num_topics}\")\n",
    "        Path(run_dir).mkdir(parents=True, exist_ok=True)\n",
    "        topics, doc_topics = train_topic_model(training_docs, NUM_TOPICS, run_dir)\n",
    "        fn_list_path = os.path.join(run_dir, \"input_filenames.txt\")\n",
    "        with open(fn_list_path, \"w\", encoding=\"utf-8\") as f:\n",
    "            for fn in target_files:\n",
    "                f.write(fn + \"\\n\")\n",
    "        excel_results = os.path.join(run_dir, \"topic_model_results.xlsx\")\n",
    "        save_results_to_excel(excel_results, topics, token_distributions, doc_topics, target_files)\n",
    "        doc_titles = {doc: Path(fn).stem for doc, fn in zip(training_docs, target_files)}\n",
    "        top_titles_path = os.path.join(run_dir, \"top_titles.xlsx\")\n",
    "        save_top_titles_excel(\n",
    "            top_titles_path, topics, training_docs, doc_topics, doc_titles, n_top_docs\n",
    "        )\n",
    "        print(f\"✅ Finished {num_topics} topics. Results in {run_dir}.\")\n",
    "    print(\"\\nAll requested topic models completed!\")\n",
    "    print(\"To create heatmaps or boxplots, import and run plotting_helpers.generate_heatmap() or generate_boxplots().\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20a91d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Searching for topic model run directories...\n",
      "Please select a run directory:\n",
      "[1] July1325/topics_13\n",
      "[2] July1325/topics_8\n",
      "[3] July1325/topics_12\n",
      "[4] July1325/topics_10\n",
      "[5] July1325/topics_11\n",
      "[6] July1325/topics_9\n",
      "Successfully loaded run directory 'July1325/topics_9' with 9 topics.\n",
      "\n",
      "Select which plots to generate:\n",
      "1) Heatmap (PDF)\n",
      "2) Boxplot grid (JPG)\n",
      "3) LMW boxplots (PDF, label-based)\n",
      "Type any combination of these numbers separated by commas (e.g. 1,2,3), or 'all' for all:\n",
      "Successfully loaded run directory 'July1325/topics_9' with 9 topics.\n",
      "\n",
      "Select which plots to generate:\n",
      "1) Heatmap (PDF)\n",
      "2) Boxplot grid (JPG)\n",
      "3) LMW boxplots (PDF, label-based)\n",
      "Type any combination of these numbers separated by commas (e.g. 1,2,3), or 'all' for all:\n",
      "Boxplot grid saved: /workspaces/Jerusalimiec-Dissertation/Text/Corpora/Concatenated/lemmatized/July1325/topics_9/all_topic_boxplots.jpg\n",
      "Boxplot grid saved: /workspaces/Jerusalimiec-Dissertation/Text/Corpora/Concatenated/lemmatized/July1325/topics_9/all_topic_boxplots.jpg\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import little_mallet_wrapper\n",
    "\n",
    "# Defaults for plot dimensions and output\n",
    "DEFAULT_DPI = 300\n",
    "DEFAULT_DIM_INCHES = (13, 9)  # (width in inches, height in inches)\n",
    "OUTPUT_DIR = None  # Will be set by user input\n",
    "NUM_TOPICS = None  # Will be set by user input\n",
    "\n",
    "# Helper function to load model results from the output directory\n",
    "# Loads topic keys, document-topic distributions, and filenames used in modeling\n",
    "# Returns: (list of filenames, list of topics, list of doc-topic distributions)\n",
    "def _load_model_results():\n",
    "    \"\"\"\n",
    "    Load topic keys, distributions, and filenames from the last pipeline run.\n",
    "    \"\"\"\n",
    "    if not OUTPUT_DIR or not NUM_TOPICS:\n",
    "        raise RuntimeError(\n",
    "            \"OUTPUT_DIR and NUM_TOPICS must be set at module level. \"\n",
    "            \"Define them before calling plotting functions.\"\n",
    "        )\n",
    "\n",
    "    key_file = os.path.join(OUTPUT_DIR, f\"mallet.topic_keys.{NUM_TOPICS}\")\n",
    "    dist_file = os.path.join(OUTPUT_DIR, f\"mallet.topic_distributions.{NUM_TOPICS}\")\n",
    "    topics = little_mallet_wrapper.load_topic_keys(key_file)\n",
    "    doc_topics = little_mallet_wrapper.load_topic_distributions(dist_file)\n",
    "\n",
    "    fn_list = os.path.join(OUTPUT_DIR, \"input_filenames.txt\")\n",
    "    with open(fn_list, \"r\", encoding=\"utf-8\") as f:\n",
    "        files = [line.strip() for line in f]\n",
    "\n",
    "    return files, topics, doc_topics\n",
    "\n",
    "# Generate and save a heatmap of topic-by-document probabilities\n",
    "# Prompts user for width, calculates figure size, and saves as PDF\n",
    "# Uses little_mallet_wrapper's plotting function\n",
    "# Only works if OUTPUT_DIR and NUM_TOPICS are set and model results exist\n",
    "def generate_heatmap():\n",
    "    \"\"\"\n",
    "    Prompt for a desired chart width in pixels, then generate and save a\n",
    "    topic-by-document heatmap. Exports only PDF + PNG.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        width_px = int(input(\"Enter heatmap width in pixels (e.g. 1600): \").strip())\n",
    "    except ValueError:\n",
    "        width_px = int(DEFAULT_DIM_INCHES[0] * DEFAULT_DPI)\n",
    "        print(f\"Invalid input, defaulting to {width_px} px width ({DEFAULT_DPI} DPI).\")\n",
    "\n",
    "    dpi_value = DEFAULT_DPI\n",
    "    aspect = DEFAULT_DIM_INCHES[1] / DEFAULT_DIM_INCHES[0]\n",
    "    height_px = int(width_px * aspect)\n",
    "    figsize = (width_px / dpi_value, height_px / dpi_value)\n",
    "\n",
    "    files, topics, doc_topics = _load_model_results()\n",
    "    labels = [Path(fn).stem for fn in files]\n",
    "\n",
    "    plt.close('all')\n",
    "    pdf_out = os.path.join(OUTPUT_DIR, \"categories_by_topics.pdf\")\n",
    "    fig = little_mallet_wrapper.plot_categories_by_topics_heatmap(\n",
    "        labels,\n",
    "        doc_topics,\n",
    "        topics,\n",
    "        pdf_out,\n",
    "        target_labels=labels,\n",
    "        dim=figsize\n",
    "    )\n",
    "\n",
    "    if fig is None:\n",
    "        fig = plt.gcf()\n",
    "\n",
    "    fig.canvas.draw()\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Heatmap saved: {pdf_out}\")\n",
    "\n",
    "# Generate and save a grid of boxplots (one subplot per topic)\n",
    "# Prompts user for width, calculates figure size, and saves as JPG\n",
    "# Each subplot shows the distribution of topic probabilities across documents\n",
    "def generate_boxplot_grid():\n",
    "    \"\"\"\n",
    "    Prompt for a desired chart width in pixels, then draw a grid of boxplots\n",
    "    (one subplot per topic) and save as a single PDF + PNG.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        width_px = int(input(\"Enter boxplot-grid width in pixels (e.g. 1600): \").strip())\n",
    "    except ValueError:\n",
    "        width_px = int(DEFAULT_DIM_INCHES[0] * DEFAULT_DPI)\n",
    "        print(f\"Invalid input, defaulting to {width_px} px width ({DEFAULT_DPI} DPI).\")\n",
    "\n",
    "    dpi_value = DEFAULT_DPI\n",
    "    aspect = DEFAULT_DIM_INCHES[1] / DEFAULT_DIM_INCHES[0]\n",
    "    height_px = int(width_px * aspect)\n",
    "    figsize = (width_px / dpi_value, height_px / dpi_value)\n",
    "\n",
    "    files, topics, doc_topics = _load_model_results()\n",
    "    labels = [Path(fn).stem for fn in files]\n",
    "    n_topics = len(topics)\n",
    "\n",
    "    cols = 2\n",
    "    rows = (n_topics + cols - 1) // cols\n",
    "\n",
    "    plt.close('all')\n",
    "    fig, axes = plt.subplots(rows, cols,\n",
    "                             figsize=figsize,\n",
    "                             dpi=dpi_value,\n",
    "                             sharex=True)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for t_idx in range(n_topics):\n",
    "        ax = axes[t_idx]\n",
    "        data = [doc_topics[i][t_idx] for i in range(len(doc_topics))]\n",
    "        ax.boxplot(data, vert=False)\n",
    "        ax.set_title(f\"Topic {t_idx}\")\n",
    "        ax.set_yticks([])\n",
    "\n",
    "    for ax in axes[n_topics:]:\n",
    "        ax.axis(\"off\")\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    png_path = os.path.join(OUTPUT_DIR, \"all_topic_boxplots.jpg\")\n",
    "    fig.savefig(png_path, dpi=dpi_value)\n",
    "    plt.close(fig)\n",
    "\n",
    "    print(f\"Boxplot grid saved: {png_path}\")\n",
    "\n",
    "# Generate and save one boxplot per topic using little_mallet_wrapper's function\n",
    "# Prompts user for width, calculates figure size, and saves each as a PDF\n",
    "# Each plot shows the distribution of a single topic across all documents\n",
    "def generate_lmw_boxplots():\n",
    "    \"\"\"\n",
    "    Prompt for a desired chart width in pixels, then generate and save\n",
    "    boxplots for each topic using lmw's plot_categories_by_topic_boxplots.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        width_px = int(input(\"Enter LMW boxplot width in pixels (e.g. 1600): \").strip())\n",
    "    except ValueError:\n",
    "        width_px = int(DEFAULT_DIM_INCHES[0] * DEFAULT_DPI)\n",
    "        print(f\"Invalid input, defaulting to {width_px} px width ({DEFAULT_DPI} DPI).\")\n",
    "\n",
    "    dpi_value = DEFAULT_DPI\n",
    "    aspect = DEFAULT_DIM_INCHES[1] / DEFAULT_DIM_INCHES[0]\n",
    "    height_px = int(width_px * aspect)\n",
    "    figsize = (width_px / dpi_value, height_px / dpi_value)\n",
    "\n",
    "    files, topics, doc_topics = _load_model_results()\n",
    "    labels = [Path(fn).stem for fn in files]\n",
    "    n_topics = len(topics)\n",
    "\n",
    "    plt.close('all')\n",
    "    out_paths = []\n",
    "    for topic_idx in range(n_topics):\n",
    "        out_path = os.path.join(\n",
    "            OUTPUT_DIR, f\"lmw_topic_boxplots_topic_{topic_idx}.pdf\"\n",
    "        )\n",
    "        fig = little_mallet_wrapper.plot_categories_by_topic_boxplots(\n",
    "            labels,\n",
    "            doc_topics,\n",
    "            topics,\n",
    "            topic_idx,  # <--- This is the missing argument!\n",
    "            output_path=out_path,\n",
    "            target_labels=None,\n",
    "            dim=figsize\n",
    "        )\n",
    "        if fig is None:\n",
    "            fig = plt.gcf()\n",
    "        fig.canvas.draw()\n",
    "        plt.close(fig)\n",
    "        out_paths.append(out_path)\n",
    "\n",
    "    print(f\"LMW boxplots saved: {', '.join(out_paths)}\")\n",
    "    \n",
    "# Main entry point for plotting: prompts user for output folder, number of topics, and which plots to generate\n",
    "def main():\n",
    "    global OUTPUT_DIR, NUM_TOPICS\n",
    "\n",
    "    # --- MODIFIED ---\n",
    "    # Helper to find all possible run directories by searching for key files.\n",
    "    def find_run_dirs(root='.'):\n",
    "        run_dirs = []\n",
    "        for r, _, files in os.walk(root):\n",
    "            if any(f.startswith(\"mallet.topic_keys.\") for f in files):\n",
    "                run_dirs.append(r)\n",
    "        return run_dirs\n",
    "\n",
    "    # Let user choose from available run directories\n",
    "    print(\"Searching for topic model run directories...\")\n",
    "    possible_dirs = find_run_dirs(os.getcwd())\n",
    "\n",
    "    if not possible_dirs:\n",
    "        print(\"No topic model run directories found in this folder or its subfolders.\")\n",
    "        return\n",
    "\n",
    "    print(\"Please select a run directory:\")\n",
    "    for i, d in enumerate(possible_dirs, 1):\n",
    "        print(f\"[{i}] {os.path.relpath(d)}\")\n",
    "\n",
    "    choice = -1\n",
    "    while not (1 <= choice <= len(possible_dirs)):\n",
    "        try:\n",
    "            choice = int(input(f\"Enter number (1-{len(possible_dirs)}): \").strip())\n",
    "        except ValueError:\n",
    "            print(\"Invalid input.\")\n",
    "\n",
    "    OUTPUT_DIR = possible_dirs[choice - 1]\n",
    "\n",
    "    # Infer number of topics from the chosen directory's files\n",
    "    try:\n",
    "        topic_keys_file = next(f for f in os.listdir(OUTPUT_DIR) if f.startswith(\"mallet.topic_keys.\"))\n",
    "        NUM_TOPICS = int(topic_keys_file.split('.')[-1])\n",
    "        print(f\"Successfully loaded run directory '{os.path.relpath(OUTPUT_DIR)}' with {NUM_TOPICS} topics.\")\n",
    "    except (StopIteration, ValueError):\n",
    "        print(\"Could not automatically determine the number of topics from the files.\")\n",
    "        num_topics_str = input(\"Please enter the number of topics manually: \").strip()\n",
    "        NUM_TOPICS = int(num_topics_str)\n",
    "    # --- END MODIFIED ---\n",
    "\n",
    "    # Prompt user to select which plots to generate\n",
    "    print(\"\\nSelect which plots to generate:\")\n",
    "    print(\"1) Heatmap (PDF)\")\n",
    "    print(\"2) Boxplot grid (JPG)\")\n",
    "    print(\"3) LMW boxplots (PDF, label-based)\")\n",
    "    print(\"Type any combination of these numbers separated by commas (e.g. 1,2,3), or 'all' for all:\")\n",
    "    choice = input(\"Your choice: \").lower()\n",
    "\n",
    "    if \"all\" in choice:\n",
    "        generate_heatmap()\n",
    "        generate_boxplot_grid()\n",
    "        generate_lmw_boxplots()\n",
    "    else:\n",
    "        if '1' in choice:\n",
    "            generate_heatmap()\n",
    "        if '2' in choice:\n",
    "            generate_boxplot_grid()\n",
    "        if '3' in choice:\n",
    "            generate_lmw_boxplots()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7595d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Prompt the user to select a MALLET run folder (subdirectory containing 'input_filenames.txt').\n",
    "# Returns the full path to the selected folder.\n",
    "def choose_mallet_run(prompt=\"Select a MALLET run folder that contains 'input_filenames.txt' and distributions files:\"):\n",
    "    \"\"\"\n",
    "    Let the user pick a subfolder containing 'input_filenames.txt'.\n",
    "    Returns the full path to that folder.\n",
    "    \"\"\"\n",
    "    cwd = os.getcwd()\n",
    "    # List all subdirectories except .ipynb_checkpoints\n",
    "    subdirs = [\n",
    "        d for d in os.listdir(cwd)\n",
    "        if os.path.isdir(os.path.join(cwd, d)) and d != \".ipynb_checkpoints\"\n",
    "    ]\n",
    "    if not subdirs:\n",
    "        print(\"No subfolders found.\")\n",
    "        return cwd\n",
    "\n",
    "    print(prompt)\n",
    "    for i, fn in enumerate(subdirs, start=1):\n",
    "        print(f\"[{i}] {fn}\")\n",
    "    # Prompt until a valid selection is made\n",
    "    while True:\n",
    "        try:\n",
    "            choice = int(input(\"Enter number: \").strip())\n",
    "            if 1 <= choice <= len(subdirs):\n",
    "                return os.path.join(cwd, subdirs[choice - 1])\n",
    "        except ValueError:\n",
    "            pass\n",
    "        print(\"Invalid choice, please try again.\")\n",
    "\n",
    "# Load the list of filenames (in modeling order) and the topic distributions\n",
    "# for a given MALLET run and number of topics.\n",
    "def load_model_data(run_dir, num_topics):\n",
    "    \"\"\"\n",
    "    Load the file list (input_filenames.txt) and the doc-topic distributions\n",
    "    for the specified number of topics from 'run_dir'.\n",
    "    Ensures that the order of files matches how they were used in modeling.\n",
    "    \"\"\"\n",
    "    fn_list_path = os.path.join(run_dir, \"input_filenames.txt\")\n",
    "    if not os.path.exists(fn_list_path):\n",
    "        raise FileNotFoundError(f\"'input_filenames.txt' not found in {run_dir}.\")\n",
    "\n",
    "    # Read the filenames in the order used for modeling\n",
    "    with open(fn_list_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        files = [line.strip() for line in f]\n",
    "\n",
    "    dist_file = os.path.join(run_dir, f\"mallet.topic_distributions.{num_topics}\")\n",
    "    if not os.path.exists(dist_file):\n",
    "        raise FileNotFoundError(f\"No doc-topic distribution file for {num_topics} topics in {run_dir}.\")\n",
    "\n",
    "    import little_mallet_wrapper\n",
    "    # Load the topic distributions for each file\n",
    "    doc_topics = little_mallet_wrapper.load_topic_distributions(dist_file)\n",
    "    return files, doc_topics\n",
    "\n",
    "# Prompt the user to select files by index, range, or 'all'.\n",
    "# Returns a sorted list of selected indices (0-based).\n",
    "def select_indices_in_files(file_list, group_name):\n",
    "    \"\"\"\n",
    "    Prompt the user to pick files by:\n",
    "      - 'all'\n",
    "      - single indices (e.g. '3')\n",
    "      - index ranges (e.g. '2-5')\n",
    "      or any combination separated by commas (e.g. '1,2-4,7').\n",
    "\n",
    "    Returns a sorted list of chosen indices (0-based).\n",
    "    \"\"\"\n",
    "    print(f\"\\nSelect indices for {group_name}. Possible choices:\")\n",
    "    for i, f in enumerate(file_list, start=1):\n",
    "        print(f\"[{i}] {f}\")\n",
    "    user_input = input(\n",
    "        f\"Enter comma-separated indices or ranges, or 'all' for {group_name}: \"\n",
    "    ).strip().lower()\n",
    "\n",
    "    if user_input == \"all\":\n",
    "        return list(range(len(file_list)))\n",
    "\n",
    "    chosen = []\n",
    "    for part in user_input.split(\",\"):\n",
    "        part = part.strip()\n",
    "        if \"-\" in part:\n",
    "            # Handle range syntax \"start-end\"\n",
    "            try:\n",
    "                start_str, end_str = part.split(\"-\")\n",
    "                start = int(start_str.strip())\n",
    "                end = int(end_str.strip())\n",
    "                if start <= end:\n",
    "                    chosen.extend(range(start - 1, end))\n",
    "                else:\n",
    "                    # If user reversed them accidentally, handle gracefully:\n",
    "                    chosen.extend(range(end - 1, start))\n",
    "            except ValueError:\n",
    "                print(f\"Ignoring invalid range: '{part}'\")\n",
    "        else:\n",
    "            # Handle single numeric index\n",
    "            try:\n",
    "                idx = int(part) - 1\n",
    "                chosen.append(idx)\n",
    "            except ValueError:\n",
    "                print(f\"Ignoring invalid entry: '{part}'\")\n",
    "\n",
    "    # Remove duplicates and ensure ascending order\n",
    "    final_indices = sorted(set(i for i in chosen if 0 <= i < len(file_list)))\n",
    "    return final_indices\n",
    "\n",
    "# Compute the average topic distance (L1 norm) between two topic distributions.\n",
    "def average_topic_distance(dist_a, dist_b):\n",
    "    \"\"\"\n",
    "    Calculate the average distance between two topic distributions\n",
    "    by summing absolute differences and dividing by the number of topics.\n",
    "    \"\"\"\n",
    "    return sum(abs(a - b) for a, b in zip(dist_a, dist_b)) / len(dist_a)\n",
    "\n",
    "# Main logic for comparing two groups of files:\n",
    "# 1. Prompt user to select two groups of files.\n",
    "# 2. Prompt user to select one file from Group 1.\n",
    "# 3. Compare that file to all files in Group 2 and rank by similarity.\n",
    "def compare_two_groups(files, doc_topics):\n",
    "    \"\"\"\n",
    "    1) Divide files into two groups, picking indices from 'files'.\n",
    "    2) Select one file from Group 1.\n",
    "    3) Compare to every file in Group 2 and rank by average topic distance.\n",
    "    \"\"\"\n",
    "    # 1) Split into two groups\n",
    "    group1_indices = select_indices_in_files(files, \"Group 1\")\n",
    "    group2_indices = select_indices_in_files(files, \"Group 2\")\n",
    "\n",
    "    # 2) Pick one file from Group 1\n",
    "    print(\"\\nWhich file from Group 1 would you like to compare?\")\n",
    "    for i, idx in enumerate(group1_indices, start=1):\n",
    "        print(f\"[{i}] {files[idx]}\")\n",
    "    choice = int(input(\"Enter the number: \")) - 1\n",
    "    chosen_idx = group1_indices[choice]\n",
    "\n",
    "    chosen_dist = doc_topics[chosen_idx]\n",
    "    print(f\"\\nSelected file from Group 1: {files[chosen_idx]}\")\n",
    "\n",
    "    # 3) For each file in Group 2, compute average topic distance, then sort\n",
    "    distances = []\n",
    "    for idx in group2_indices:\n",
    "        dist = average_topic_distance(chosen_dist, doc_topics[idx])\n",
    "        distances.append((files[idx], dist))\n",
    "\n",
    "    # Sort by ascending distance (most similar = smallest distance)\n",
    "    distances.sort(key=lambda x: x[1])\n",
    "\n",
    "    print(\"\\nRanking by similarity (ascending distance):\")\n",
    "    for rank, (fname, distval) in enumerate(distances, start=1):\n",
    "        print(f\"{rank}. {fname} (distance={distval:.4f})\")\n",
    "\n",
    "# Entry point for the comparison workflow:\n",
    "# 1. Prompt user to select a MALLET run folder and number of topics.\n",
    "# 2. Load the files and topic distributions.\n",
    "# 3. Launch the two-group comparison.\n",
    "def start_comparison():\n",
    "    \"\"\"\n",
    "    Guide the user to pick a MALLET run folder, specify the number of topics,\n",
    "    then load files and doc_topic distributions and compare two groups.\n",
    "    \"\"\"\n",
    "    run_dir = choose_mallet_run()\n",
    "    num_topics = int(input(\"Enter the number of topics used for that run: \").strip())\n",
    "    files, doc_topics = load_model_data(run_dir, num_topics)\n",
    "    compare_two_groups(files, doc_topics)\n",
    "\n",
    "# Run the comparison workflow when this cell is executed\n",
    "start_comparison()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

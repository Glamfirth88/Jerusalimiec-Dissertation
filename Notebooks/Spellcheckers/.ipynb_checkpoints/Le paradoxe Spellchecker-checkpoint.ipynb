{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcd07665-38bc-4fed-8621-4a04acb65976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import glob, a module that helps with file management.\n",
    "import glob\n",
    "import pandas as pd\n",
    "from spellchecker import SpellChecker\n",
    "from pathlib import Path\n",
    "from nltk import wordpunct_tokenize\n",
    "from collections import Counter\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27b7fc5d-c697-4ef5-ac4d-c6345a5936cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text files to be spellchecked: ['Le paradoxe.txt']\n"
     ]
    }
   ],
   "source": [
    "#doc_name = 'Démonomanie'\n",
    "outputpath = \"./final\"\n",
    "outputfile_path = Path(outputpath)\n",
    "outputfile_path.mkdir(exist_ok=True)\n",
    "texts_folder = Path.cwd()\n",
    "doc_name = texts_folder.name\n",
    "texts_list = glob.glob(\"*.txt\")\n",
    "print(\"Text files to be spellchecked:\", texts_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8251a74-964b-47ac-9752-9d785cb06cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spellchecker dictionary.\n",
    "# Replace the language attribute with another 2 letter code\n",
    "# to select another language. Options are: English - ‘en’, Spanish - ‘es’,\n",
    "# French - ‘fr’, Portuguese - ‘pt’, German - ‘de’, Russian - ‘ru’.\n",
    "\n",
    "spell = SpellChecker(language='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d411299b-c6ea-4ac9-9bc5-97628f5fcff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lucas-jerusalimiec/Documents/OCR Text/Text/Bodin/Le paradoxe/Le paradoxe.txt checked for readability.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>token_count</th>\n",
       "      <th>unknown_count</th>\n",
       "      <th>readability</th>\n",
       "      <th>unknown_words</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>placeholder</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>placeholder</td>\n",
       "      <td>placeholder</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/lucas-jerusalimiec/Documents/OCR Text/Te...</td>\n",
       "      <td>16319</td>\n",
       "      <td>3811</td>\n",
       "      <td>76.65</td>\n",
       "      <td>{décenraifon, yautre, mcs, recouurer, nousauós...</td>\n",
       "      <td>\\n\\n    LE PARADOXE MORAL \\nDE. JEAN BODIN ANG...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_name  token_count  \\\n",
       "0                                        placeholder            0   \n",
       "0  /home/lucas-jerusalimiec/Documents/OCR Text/Te...        16319   \n",
       "\n",
       "   unknown_count  readability  \\\n",
       "0              0         0.00   \n",
       "0           3811        76.65   \n",
       "\n",
       "                                       unknown_words  \\\n",
       "0                                        placeholder   \n",
       "0  {décenraifon, yautre, mcs, recouurer, nousauós...   \n",
       "\n",
       "                                                text  \n",
       "0                                        placeholder  \n",
       "0  \\n\\n    LE PARADOXE MORAL \\nDE. JEAN BODIN ANG...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Dictionary Test a Folder of .txt Files ###\n",
    "\n",
    "# We'll use Pandas to create a dataframe (a table) that can hold \n",
    "# information about an OCR'ed page and display it in a tabular format.\n",
    "# This dataframe will start out empty with only its column headers \n",
    "# defined. We'll add information to it one page at a time. So each\n",
    "# row will represent 1 page.\n",
    "\n",
    "placeholder_values = ['placeholder', 0, 0, 0, 'placeholder', 'placeholder']\n",
    "\n",
    "df = pd.DataFrame([placeholder_values], columns=[\"file_name\",\"token_count\",\"unknown_count\",\"readability\",\"unknown_words\",\"text\"])\n",
    "\n",
    "# Set the folder for the input images\n",
    "\n",
    "for txt_file in texts_folder.glob('*.txt'):\n",
    "    \n",
    "    # Open each text file and read text into `ocrText`\n",
    "    with open(txt_file, 'r') as inputFile:\n",
    "        ocrText = inputFile.read()\n",
    "        \n",
    "    # Join hyphenated words that are split between lines by \n",
    "    # looking for a hyphen followed by a newline character: \"-\\n\"\n",
    "    # \"\\n\" is an \"escape character\" and represents the \n",
    "    # \"newline,\" a character that is usually invisible \n",
    "    # to human readers but that computers use to mark the \n",
    "    # end/beginning of a line. Each time you press the \n",
    "    # Enter/Return key on your keyboard, an invisible \"\\n\" \n",
    "    # is created to mark the beginning of a new line.\n",
    "    ocrText = ocrText.replace(\"-\\n\",\"\")\n",
    "    \n",
    "    # First, we'll use NLTK to \"tokenize\" text. \n",
    "            # \"Tokenize\" here means to take a page of our OCR'ed text,\n",
    "            # which Python is currently reading as one big glob of data,\n",
    "            # and separate each word out so that it can be read as an\n",
    "            # individual piece of data within a larger data structure \n",
    "            # (a list). This process also removes punctuation.\n",
    "    tokens = wordpunct_tokenize(ocrText)\n",
    "    \n",
    "    # Lowercase all tokens\n",
    "    tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "    \n",
    "    # Now we can get all of the words that don't match the \n",
    "    # spellchecker dictionary or our list of place names--\n",
    "    # these are the potential spelling errors.\n",
    "    unknown = spell.unknown(tokens)\n",
    "    \n",
    "    # Let's use a little math to find out how many potential \n",
    "    # spelling errors were identified. As part of this process, \n",
    "    # we'll create a \"readability\" score that will give us a \n",
    "    # percentage of how readable each file is--how much of the \n",
    "    # OCR'ed is \"correct.\"\n",
    "        \n",
    "    # If the list of unknown tokens (words) is greater than 0 \n",
    "    # (i.e. if the list is not empty):\n",
    "    if len(unknown) != 0:\n",
    "            \n",
    "               # Following order of operations, here's what's happening \n",
    "               # in the readability variable below:\n",
    "               # 1. Divide the number of unknown tokens (len(unknown)) \n",
    "                    # by the total number of tokens on the page\n",
    "                    # (len(tokens)). Use \"float\" to specify that Python\n",
    "                    # returns a decimal number:\n",
    "                        # (float(len(unknown))/float(len(tokens))\n",
    "               # 2. Multiply the number from step 1 by 100.\n",
    "                    # (float(len(unknown))/float(len(tokens)) * 100)\n",
    "               # 3. Subtract the number from step 2 from 100.\n",
    "                    # 100 - (float(len(unknown))/float(len(tokens)) * 100)\n",
    "               # 4. Round the number from step 3 to 2 decimal places\n",
    "                    # round(100 - (float(len(unknown))/float(len(tokens)) * 100), 2)\n",
    "            \n",
    "        readability = round(100 - (float(len(unknown))/float(len(tokens)) * 100), 2)\n",
    "        \n",
    "        # If the list of unknown tokens is empty (or equal to 0), then readability is 100!\n",
    "    else:\n",
    "        readability = 100\n",
    "    \n",
    "    # Let's create a record of the readability information \n",
    "    # for this page that we'll add to the dataframe. \n",
    "    # The following is a Python dictionary, another way of \n",
    "    # storing data. Each word or phrase to the left of the : is a\n",
    "    # \"key\" -- think of it as a column header. Each piece of \n",
    "    # information to the right is a \"value\" -- information \n",
    "    # written in a single cell below each header. \n",
    "\n",
    "    df2 = pd.DataFrame({\n",
    "            \"file_name\" : txt_file.as_posix(),\n",
    "            \"token_count\" : len(tokens),\n",
    "            \"unknown_count\" : len(unknown),\n",
    "            \"readability\" : readability,\n",
    "            \"unknown_words\" : [unknown],\n",
    "            \"text\" : ocrText\n",
    "            })\n",
    "\n",
    "    df = pd.concat([df, df2])\n",
    "\n",
    "    # This statement lets us know if a page has been successfully \n",
    "    # checked for readability.\n",
    "    print(txt_file, \"checked for readability.\")\n",
    "    \n",
    "# This time, instead of creating individual .txt files for each page,\n",
    "# we're going to save all of the OCR'ed text and readability \n",
    "# information to a single .csv (\"comma separated value\") file. \n",
    "# We can view this file format as a table. Having everything stored \n",
    "# like this will help us with clean up and future analysis.\n",
    "df.to_csv('spellcheck_data.csv', header=True, index=False, sep=',')\n",
    "\n",
    "# We have the data stored in a file now, but we can also \n",
    "# preview it here:\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30da9866-7585-4a98-973a-601c3c991bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_words = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0', 'magnani-', '\\nuerain', 'uarain', 'fou-', ' mité', '\\nmité']\n",
    "known_words = ['', '', '', '', '', '', '', '', '', '', 'magnanimité', 'fouuerain', 'fouuerain', '', '', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c060d7ac-da17-45e2-9057-81654275a420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All instances of 1 replaced with .\n",
      "All instances of 2 replaced with .\n",
      "All instances of 3 replaced with .\n",
      "All instances of 4 replaced with .\n",
      "All instances of 5 replaced with .\n",
      "All instances of 6 replaced with .\n",
      "All instances of 7 replaced with .\n",
      "All instances of 8 replaced with .\n",
      "All instances of 9 replaced with .\n",
      "All instances of 0 replaced with .\n",
      "All instances of magnani- replaced with magnanimité.\n",
      "All instances of \n",
      "uerain replaced with fouuerain.\n",
      "All instances of uarain replaced with fouuerain.\n",
      "All instances of fou- replaced with .\n",
      "All instances of  mité replaced with .\n",
      "All instances of \n",
      "mité replaced with .\n"
     ]
    }
   ],
   "source": [
    "# Identify the sample_output file path.\n",
    "# Remember that our readability output is also stored \n",
    "# in this file as a .csv. We don't want to change it, \n",
    "# so we'll use glob to look for only .txt files.\n",
    "outputfile = f'{outputfile_path.as_posix()}/{doc_name}_corrected.txt'\n",
    "\n",
    "# Apply the following loop to one file at a time in filePath.\n",
    "for file in texts_folder.glob('*.txt'):\n",
    "    \n",
    "    # Open a file in \"read\" (r) mode.\n",
    "    text = open(file, \"r\")\n",
    "    \n",
    "    # Read in the contents of that file.\n",
    "    word_correction = text.read()\n",
    "\n",
    "    word_correction = word_correction.lower()\n",
    "    \n",
    "    # Find instances of and unknown word and replace\n",
    "    # with a known word\n",
    "    for i in range(len(known_words)):\n",
    "          \n",
    "        unknown_word = unknown_words[i]\n",
    "    \n",
    "        known_word = known_words[i]\n",
    "\n",
    "        word_correction = word_correction.replace(unknown_word, known_word)\n",
    "\n",
    "        print(\"All instances of \" + unknown_word + \" replaced with \" + known_word + \".\")    \n",
    " \n",
    "    # Reopen the file in \"write\" (w) mode.\n",
    "    file = open(outputfile, \"w\")\n",
    "    \n",
    "    # Add the changed word into the reopened file.\n",
    "    file.write(word_correction)\n",
    "    \n",
    "    # Close the file.\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e7ee69e-4202-404d-92c1-7d9f7b61ab79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spellchecked Le paradoxe_corrected.txt. Readability = 82.46\n"
     ]
    }
   ],
   "source": [
    "# Process each .txt file in the folder\n",
    "for filename in os.listdir(outputpath):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(outputpath, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text_data = file.read()\n",
    "\n",
    "        text_data = text_data.replace(\"-\\n\",\"\")\n",
    "    \n",
    "        words = wordpunct_tokenize(text_data)\n",
    "    \n",
    "        misspelled = spell.unknown(words)\n",
    "\n",
    "        if len(misspelled) != 0:\n",
    "                                \n",
    "            readability = round(100 - (float(len(misspelled))/float(len(words)) * 100), 2)\n",
    "        \n",
    "        # If the list of unknown tokens is empty (or equal to 0), then readability is 100!\n",
    "        else:\n",
    "            readability = 100\n",
    "\n",
    "        # Count the frequency of each misspelled word\n",
    "        word_counts = Counter(misspelled)\n",
    "\n",
    "        # SCreate a DataFrame and sort by frequency\n",
    "        misspelled_df = pd.DataFrame(word_counts.items(), columns=['word', 'count'])\n",
    "        misspelled_df = misspelled_df.sort_values(by='count', ascending=False)\n",
    "\n",
    "        # Save to a CSV file named after the .txt file\n",
    "        csv_filename = os.path.splitext(filename)[0] + '_spellechecker.csv'\n",
    "        misspelled_df.to_csv(os.path.join(outputpath, csv_filename), index=False)\n",
    "        \n",
    "        print(f'Spellchecked {filename}. Readability = {readability}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

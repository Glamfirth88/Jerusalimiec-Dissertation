{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dcd07665-38bc-4fed-8621-4a04acb65976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import glob, a module that helps with file management.\n",
    "import glob\n",
    "import pandas as pd\n",
    "from spellchecker import SpellChecker\n",
    "from pathlib import Path\n",
    "from nltk import wordpunct_tokenize\n",
    "from collections import Counter\n",
    "import os\n",
    "import re\n",
    "from openpyxl import load_workbook\n",
    "from openpyxl.styles import Alignment, Font\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "27b7fc5d-c697-4ef5-ac4d-c6345a5936cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text files to be spellchecked: ['Le paradoxe.txt']\n",
      "doc_name = 'Le paradoxe'\n"
     ]
    }
   ],
   "source": [
    "# Define the output path and create the directory if it doesn't exist\n",
    "outputpath = \"./final\"\n",
    "outputfile_path = Path(outputpath)\n",
    "outputfile_path.mkdir(exist_ok=True)\n",
    "\n",
    "# Get the current working directory\n",
    "texts_folder = Path.cwd()\n",
    "\n",
    "# Find all .txt files in the current directory\n",
    "texts_list = glob.glob(\"*.txt\")\n",
    "print(\"Text files to be spellchecked:\", texts_list)\n",
    "\n",
    "# Set doc_name to the name of the first .txt file found in texts_list (without the extension)\n",
    "if texts_list:\n",
    "    doc_name = Path(texts_list[0]).stem\n",
    "else:\n",
    "    doc_name = None\n",
    "\n",
    "print(f\"doc_name = '{doc_name}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8251a74-964b-47ac-9752-9d785cb06cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the spellchecker dictionary.\n",
    "# Replace the language attribute with another 2 letter code\n",
    "# to select another language. Options are: English - ‘en’, Spanish - ‘es’,\n",
    "# French - ‘fr’, Portuguese - ‘pt’, German - ‘de’, Russian - ‘ru’.\n",
    "\n",
    "spell = SpellChecker(language='fr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d411299b-c6ea-4ac9-9bc5-97628f5fcff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/lucas-jerusalimiec/Documents/OCR Text/Text/Bodin/Le paradoxe/Le paradoxe.txt checked for readability.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>file_name</th>\n",
       "      <th>token_count</th>\n",
       "      <th>unknown_count</th>\n",
       "      <th>readability</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/lucas-jerusalimiec/Documents/OCR Text/Te...</td>\n",
       "      <td>16319</td>\n",
       "      <td>3811</td>\n",
       "      <td>76.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           file_name  token_count  \\\n",
       "0  /home/lucas-jerusalimiec/Documents/OCR Text/Te...        16319   \n",
       "\n",
       "   unknown_count  readability  \n",
       "0           3811        76.65  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize an empty list to collect data\n",
    "data_list = []\n",
    "\n",
    "# Set the folder for the input text files\n",
    "with pd.ExcelWriter(f'{doc_name}_spellcheck_data.xlsx', engine='openpyxl') as writer:\n",
    "    for txt_file in texts_folder.glob('*.txt'):\n",
    "        \n",
    "        # Open each text file and read text into `ocrText`\n",
    "        with open(txt_file, 'r') as inputFile:\n",
    "            ocrText = inputFile.read()\n",
    "            \n",
    "        # Join hyphenated words that are split between lines\n",
    "        ocrText = ocrText.replace(\"-\\n\", \"\")\n",
    "        \n",
    "        # Tokenize the text\n",
    "        tokens = wordpunct_tokenize(ocrText)\n",
    "        \n",
    "        # Lowercase all tokens and filter out non-alphabetic tokens\n",
    "        tokens = [token.lower() for token in tokens if token.isalpha()]\n",
    "        \n",
    "        # Identify unknown words\n",
    "        unknown = spell.unknown(tokens)\n",
    "        \n",
    "        # Calculate readability\n",
    "        if len(unknown) != 0:\n",
    "            readability = round(100 - (float(len(unknown)) / float(len(tokens)) * 100), 2)\n",
    "        else:\n",
    "            readability = 100\n",
    "        \n",
    "        # Sort the unknown words first by 'counts' descending and then by 'unknown_tokens' alphabetically\n",
    "        sorted_unknown = sorted(Counter(unknown).items(), key=lambda item: (-item[1], item[0]))\n",
    "        \n",
    "        # Append the data for this file to the list\n",
    "        data_list.append({\n",
    "            \"file_name\": txt_file.as_posix(),\n",
    "            \"token_count\": len(tokens),\n",
    "            \"unknown_count\": len(unknown),\n",
    "            \"readability\": readability,\n",
    "        })\n",
    "        \n",
    "        # Write the DataFrame to a new sheet in the Excel file\n",
    "        pd.DataFrame([data_list[-1]]).to_excel(writer, sheet_name=os.path.splitext(txt_file.name)[0], index=False)\n",
    "        \n",
    "        # Write the unknown tokens and counts vertically\n",
    "        sheet = writer.sheets[os.path.splitext(txt_file.name)[0]]\n",
    "        start_row = 6  # Add two extra rows of space\n",
    "        sheet.cell(row=start_row, column=1, value=\"unknown_tokens\").font = Font(bold=True)\n",
    "        sheet.cell(row=start_row, column=2, value=\"counts\").font = Font(bold=True)\n",
    "        for i, (token, count) in enumerate(sorted_unknown, start=start_row + 1):\n",
    "            sheet.cell(row=i, column=1, value=token)\n",
    "            sheet.cell(row=i, column=2, value=count)\n",
    "        \n",
    "        # Print a message indicating the file has been processed\n",
    "        print(txt_file, \"checked for readability.\")\n",
    "    \n",
    "    # Create a DataFrame from the collected data\n",
    "    df = pd.DataFrame(data_list)\n",
    "    \n",
    "    # Write the combined DataFrame to a summary sheet\n",
    "    df.to_excel(writer, sheet_name='Summary', index=False)\n",
    "\n",
    "# Load the workbook to adjust cell alignment\n",
    "wb = load_workbook(f'{doc_name}_spellcheck_data.xlsx')\n",
    "\n",
    "# Align cells in the summary sheet\n",
    "summary_sheet = wb['Summary']\n",
    "for row in summary_sheet.iter_rows(min_row=2, max_row=summary_sheet.max_row, min_col=1, max_col=4):\n",
    "    for cell in row:\n",
    "        cell.alignment = Alignment(vertical='top')\n",
    "\n",
    "# Align cells in each individual sheet\n",
    "for sheet_name in wb.sheetnames:\n",
    "    if sheet_name != 'Summary':\n",
    "        sheet = wb[sheet_name]\n",
    "        for row in sheet.iter_rows(min_row=2, max_row=sheet.max_row, min_col=1, max_col=4):\n",
    "            for cell in row:\n",
    "                cell.alignment = Alignment(vertical='top')\n",
    "\n",
    "# Save the workbook with the updated alignment\n",
    "wb.save(f'{doc_name}_spellcheck_data.xlsx')\n",
    "\n",
    "# Preview the DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30da9866-7585-4a98-973a-601c3c991bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "unknown_words = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '0',\n",
    "                 'ainfi', 'chofe', 'chofes', 'fapience', 'fouuerain', 'fou- \\nuarain', 'fou- \\nuerain', 'magnani- \\nmité']\n",
    "known_words = ['', '', '', '', '', '', '', '', '', '',\n",
    "               'ainsi', 'chose', 'choses', 'sapience', 'souverain', 'souverain', 'souverain', 'magnanimité']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c060d7ac-da17-45e2-9057-81654275a420",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All instances of '1' replaced with ''.\n",
      "All instances of '2' replaced with ''.\n",
      "All instances of '3' replaced with ''.\n",
      "All instances of '4' replaced with ''.\n",
      "All instances of '5' replaced with ''.\n",
      "All instances of '6' replaced with ''.\n",
      "All instances of '7' replaced with ''.\n",
      "All instances of '8' replaced with ''.\n",
      "All instances of '9' replaced with ''.\n",
      "All instances of '0' replaced with ''.\n",
      "All instances of 'ainfi' replaced with 'ainsi'.\n",
      "All instances of 'chofe' replaced with 'chose'.\n",
      "All instances of 'chofes' replaced with 'choses'.\n",
      "All instances of 'fapience' replaced with 'sapience'.\n",
      "All instances of 'fouuerain' replaced with 'souverain'.\n",
      "All instances of 'fou- \n",
      "uarain' replaced with 'souverain'.\n",
      "All instances of 'fou- \n",
      "uerain' replaced with 'souverain'.\n",
      "All instances of 'magnani- \n",
      "mité' replaced with 'magnanimité'.\n"
     ]
    }
   ],
   "source": [
    "# Identify the sample_output file path.\n",
    "outputfile = f'{outputfile_path.as_posix()}/{doc_name}_corrected.txt'\n",
    "\n",
    "# Apply the following loop to one file at a time in filePath.\n",
    "for file in texts_folder.glob('*.txt'):\n",
    "    \n",
    "    # Open a file in \"read\" (r) mode.\n",
    "    text = open(file, \"r\")\n",
    "    \n",
    "    # Read in the contents of that file.\n",
    "    word_correction = text.read()\n",
    "\n",
    "    word_correction = word_correction.lower()\n",
    "    \n",
    "    # Find instances of and unknown word and replace\n",
    "    # with a known word\n",
    "    for i in range(len(known_words)):\n",
    "          \n",
    "        unknown_word = unknown_words[i]\n",
    "    \n",
    "        known_word = known_words[i]\n",
    "\n",
    "        word_correction = word_correction.replace(unknown_word, known_word)\n",
    "      \n",
    "        print(f\"All instances of '{unknown_word}' replaced with '{known_word}'.\")\n",
    " \n",
    "    # Reopen the file in \"write\" (w) mode.\n",
    "    file = open(outputfile, \"w\")\n",
    "    \n",
    "    # Add the changed word into the reopened file.\n",
    "    file.write(word_correction)\n",
    "    \n",
    "    # Close the file.\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e7ee69e-4202-404d-92c1-7d9f7b61ab79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spellchecked Le paradoxe_corrected.txt. Readability = 82.48\n"
     ]
    }
   ],
   "source": [
    "# Process each .txt file in the folder\n",
    "for filename in os.listdir(outputpath):\n",
    "    if filename.endswith('.txt'):\n",
    "        file_path = os.path.join(outputpath, filename)\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            text_data = file.read()\n",
    "\n",
    "        text_data = text_data.replace(\"-\\n\", \"\")\n",
    "    \n",
    "        words = wordpunct_tokenize(text_data)\n",
    "    \n",
    "        misspelled = spell.unknown(words)\n",
    "\n",
    "        if len(misspelled) != 0:\n",
    "            readability = round(100 - (float(len(misspelled)) / float(len(words)) * 100), 2)\n",
    "        else:\n",
    "            readability = 100\n",
    "\n",
    "        # Count the frequency of each misspelled word\n",
    "        word_counts = Counter(misspelled)\n",
    "\n",
    "        # Sort word_counts first by 'count' descending and then by 'word' alphabetically\n",
    "        sorted_word_counts = sorted(word_counts.items(), key=lambda item: (-item[1], item[0]))\n",
    "\n",
    "        # Create a DataFrame and sort by frequency\n",
    "        misspelled_df = pd.DataFrame(sorted_word_counts, columns=['word', 'count'])\n",
    "\n",
    "        # Add readability as the first row\n",
    "        readability_header = pd.DataFrame([[\"Readability\", readability]], columns=['word', 'count'])\n",
    "        blank_rows = pd.DataFrame([[\"\", \"\"]], columns=['word', 'count'])\n",
    "        header_row = pd.DataFrame([[\"word\", \"count\"]], columns=['word', 'count'])\n",
    "        misspelled_df = pd.concat([readability_header, blank_rows, header_row, misspelled_df], ignore_index=True)\n",
    "\n",
    "        # Save to a CSV file named after the .txt file\n",
    "        csv_filename = os.path.splitext(filename)[0] + '_spellchecker.csv'\n",
    "        misspelled_df.to_csv(os.path.join(outputpath, csv_filename), index=False, header=False)\n",
    "        \n",
    "        print(f'Spellchecked {filename}. Readability = {readability}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
